{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the environement\n",
    "\n",
    "On a clean environment run the following script to set up proper neuron versions\n",
    "```\n",
    "./installation.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:201: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, :seq_length]\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:1570: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape == tensor_shape for input_tensor in input_tensors\n",
      "INFO:Neuron:Optimize = None\n",
      "INFO:Neuron:Compiler args type is <class 'list'> value is ['-O2']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Use compile_from_neff function\n",
      "-- Use create_runnable function\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# USE conda_python3 environment!!!\n",
    "\n",
    "import torch\n",
    "import torch_neuron\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "sentence1=\"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"\n",
    "sentence2=\"The greatest glory in living lies not in never falling, but in rising every time we fall.\"\n",
    "sentence3=\"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success. The greatest glory in living lies not in never falling, but in rising every time we fall. If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "encoded_sentence = tokenizer.encode_plus(sentence1, sentence3, max_length=128, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "\n",
    "example_inputs = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids']\n",
    "model_neuron = torch.neuron.trace(model, example_inputs, compiler_args=['-O2'], verbose=1, compiler_workdir='./compile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neuron.save('neuron_compiled_bert_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "tar -czvf model.tar.gz neuron_compiled_bert_model.pt\n",
    "aws s3 cp model.tar.gz s3://inf1-compiled-bert-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pre compile graph\n",
      "graph(%self.1 : __torch__.transformers.modeling_bert.___torch_mangle_1934.BertModel,\n",
      "      %input_ids : Long(1, 128),\n",
      "      %attention_mask.1 : Long(1, 128),\n",
      "      %input.2 : Long(1, 128)):\n",
      "  %3386 : __torch__.transformers.modeling_bert.___torch_mangle_1933.BertPooler = prim::GetAttr[name=\"pooler\"](%self.1)\n",
      "  %3381 : __torch__.transformers.modeling_bert.___torch_mangle_1930.BertEncoder = prim::GetAttr[name=\"encoder\"](%self.1)\n",
      "  %2972 : __torch__.transformers.modeling_bert.___torch_mangle_1724.BertEmbeddings = prim::GetAttr[name=\"embeddings\"](%self.1)\n",
      "  %641 : int = prim::Constant[value=0]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %642 : int = prim::Constant[value=0]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %643 : int = prim::Constant[value=9223372036854775807]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %644 : int = prim::Constant[value=1]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %645 : Long(1, 128) = aten::slice(%attention_mask.1, %641, %642, %643, %644) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %646 : int = prim::Constant[value=1]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %647 : Long(1, 1, 128) = aten::unsqueeze(%645, %646) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %648 : int = prim::Constant[value=2]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %649 : Long(1, 1, 1, 128) = aten::unsqueeze(%647, %648) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %650 : int = prim::Constant[value=3]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %651 : int = prim::Constant[value=0]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %652 : int = prim::Constant[value=9223372036854775807]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %653 : int = prim::Constant[value=1]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %extended_attention_mask : Long(1, 1, 1, 128) = aten::slice(%649, %650, %651, %652, %653) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:258:0\n",
      "  %655 : int = prim::Constant[value=6]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:271:0\n",
      "  %656 : bool = prim::Constant[value=0]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:271:0\n",
      "  %657 : bool = prim::Constant[value=0]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:271:0\n",
      "  %658 : None = prim::Constant()\n",
      "  %659 : Float(1, 1, 1, 128) = aten::to(%extended_attention_mask, %655, %656, %657, %658) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:271:0\n",
      "  %660 : float = prim::Constant[value=1.]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/tensor.py:403:0\n",
      "  %661 : int = prim::Constant[value=1]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/tensor.py:403:0\n",
      "  %662 : Float(1, 1, 1, 128) = aten::rsub(%659, %660, %661) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/tensor.py:403:0\n",
      "  %663 : Double() = prim::Constant[value={-10000}]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:272:0\n",
      "  %attention_mask : Float(1, 1, 1, 128) = aten::mul(%662, %663) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_utils.py:272:0\n",
      "  %3604 : __torch__.torch.nn.modules.dropout.___torch_mangle_1723.Dropout = prim::GetAttr[name=\"dropout\"](%2972)\n",
      "  %3605 : __torch__.torch.nn.modules.normalization.___torch_mangle_1722.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%2972)\n",
      "  %3606 : __torch__.torch.nn.modules.sparse.___torch_mangle_1721.Embedding = prim::GetAttr[name=\"token_type_embeddings\"](%2972)\n",
      "  %3607 : __torch__.torch.nn.modules.sparse.___torch_mangle_1720.Embedding = prim::GetAttr[name=\"position_embeddings\"](%2972)\n",
      "  %3608 : __torch__.torch.nn.modules.sparse.___torch_mangle_1719.Embedding = prim::GetAttr[name=\"word_embeddings\"](%2972)\n",
      "  %3609 : Tensor = prim::GetAttr[name=\"position_ids\"](%2972)\n",
      "  %3610 : int = prim::Constant[value=1](), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:194:0\n",
      "  %3611 : int = aten::size(%input_ids, %3610), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:194:0\n",
      "  %seq_length : Long() = prim::NumToTensor(%3611), scope: __module.embeddings\n",
      "  %3613 : int = aten::Int(%seq_length), scope: __module.embeddings\n",
      "  %3614 : int = prim::Constant[value=0](), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:201:0\n",
      "  %3615 : int = prim::Constant[value=0](), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:201:0\n",
      "  %3616 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:201:0\n",
      "  %3617 : int = prim::Constant[value=1](), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:201:0\n",
      "  %3618 : Long(1, 512) = aten::slice(%3609, %3614, %3615, %3616, %3617), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:201:0\n",
      "  %3619 : int = prim::Constant[value=1](), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:201:0\n",
      "  %3620 : int = prim::Constant[value=0](), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:201:0\n",
      "  %3621 : int = prim::Constant[value=1](), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:201:0\n",
      "  %input.1 : Long(1, 128) = aten::slice(%3618, %3619, %3620, %3613, %3621), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:201:0\n",
      "  %3623 : Tensor = prim::GetAttr[name=\"weight\"](%3608)\n",
      "  %3624 : int = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %3625 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %3626 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %inputs_embeds : Float(1, 128, 768) = aten::embedding(%3623, %input_ids, %3624, %3625, %3626), scope: __module.embeddings/__module.embeddings.word_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %3628 : Tensor = prim::GetAttr[name=\"weight\"](%3607)\n",
      "  %3629 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.position_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %3630 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.position_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %3631 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.position_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %position_embeddings : Float(1, 128, 768) = aten::embedding(%3628, %input.1, %3629, %3630, %3631), scope: __module.embeddings/__module.embeddings.position_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %3633 : Tensor = prim::GetAttr[name=\"weight\"](%3606)\n",
      "  %3634 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.token_type_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %3635 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.token_type_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %3636 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.token_type_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %token_type_embeddings : Float(1, 128, 768) = aten::embedding(%3633, %input.2, %3634, %3635, %3636), scope: __module.embeddings/__module.embeddings.token_type_embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1724:0\n",
      "  %3638 : int = prim::Constant[value=1](), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:211:0\n",
      "  %3639 : Float(1, 128, 768) = aten::add(%inputs_embeds, %position_embeddings, %3638), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:211:0\n",
      "  %3640 : int = prim::Constant[value=1](), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:211:0\n",
      "  %input.3 : Float(1, 128, 768) = aten::add(%3639, %token_type_embeddings, %3640), scope: __module.embeddings # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:211:0\n",
      "  %3642 : Tensor = prim::GetAttr[name=\"bias\"](%3605)\n",
      "  %3643 : Tensor = prim::GetAttr[name=\"weight\"](%3605)\n",
      "  %3644 : int = prim::Constant[value=768](), scope: __module.embeddings/__module.embeddings.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3645 : int[] = prim::ListConstruct(%3644), scope: __module.embeddings/__module.embeddings.LayerNorm\n",
      "  %3646 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.embeddings/__module.embeddings.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3647 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.4 : Float(1, 128, 768) = aten::layer_norm(%input.3, %3645, %3643, %3642, %3646, %3647), scope: __module.embeddings/__module.embeddings.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3649 : float = prim::Constant[value=0.10000000000000001](), scope: __module.embeddings/__module.embeddings.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3650 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %input.5 : Float(1, 128, 768) = aten::dropout(%input.4, %3649, %3650), scope: __module.embeddings/__module.embeddings.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3652 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3653 : __torch__.transformers.modeling_bert.___torch_mangle_1928.BertLayer = prim::GetAttr[name=\"11\"](%3652)\n",
      "  %3654 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3655 : __torch__.transformers.modeling_bert.___torch_mangle_1911.BertLayer = prim::GetAttr[name=\"10\"](%3654)\n",
      "  %3656 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3657 : __torch__.transformers.modeling_bert.___torch_mangle_1894.BertLayer = prim::GetAttr[name=\"9\"](%3656)\n",
      "  %3658 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3659 : __torch__.transformers.modeling_bert.___torch_mangle_1877.BertLayer = prim::GetAttr[name=\"8\"](%3658)\n",
      "  %3660 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3661 : __torch__.transformers.modeling_bert.___torch_mangle_1860.BertLayer = prim::GetAttr[name=\"7\"](%3660)\n",
      "  %3662 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3663 : __torch__.transformers.modeling_bert.___torch_mangle_1843.BertLayer = prim::GetAttr[name=\"6\"](%3662)\n",
      "  %3664 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3665 : __torch__.transformers.modeling_bert.___torch_mangle_1826.BertLayer = prim::GetAttr[name=\"5\"](%3664)\n",
      "  %3666 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3667 : __torch__.transformers.modeling_bert.___torch_mangle_1809.BertLayer = prim::GetAttr[name=\"4\"](%3666)\n",
      "  %3668 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3669 : __torch__.transformers.modeling_bert.___torch_mangle_1792.BertLayer = prim::GetAttr[name=\"3\"](%3668)\n",
      "  %3670 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3671 : __torch__.transformers.modeling_bert.___torch_mangle_1775.BertLayer = prim::GetAttr[name=\"2\"](%3670)\n",
      "  %3672 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3673 : __torch__.transformers.modeling_bert.___torch_mangle_1758.BertLayer = prim::GetAttr[name=\"1\"](%3672)\n",
      "  %3674 : __torch__.torch.nn.modules.container.___torch_mangle_1929.ModuleList = prim::GetAttr[name=\"layer\"](%3381)\n",
      "  %3675 : __torch__.transformers.modeling_bert.___torch_mangle_1741.BertLayer = prim::GetAttr[name=\"0\"](%3674)\n",
      "  %3676 : __torch__.transformers.modeling_bert.___torch_mangle_1740.BertOutput = prim::GetAttr[name=\"output\"](%3675)\n",
      "  %3677 : __torch__.transformers.modeling_bert.___torch_mangle_1736.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3675)\n",
      "  %3678 : __torch__.transformers.modeling_bert.___torch_mangle_1734.BertAttention = prim::GetAttr[name=\"attention\"](%3675)\n",
      "  %3679 : __torch__.transformers.modeling_bert.___torch_mangle_1733.BertSelfOutput = prim::GetAttr[name=\"output\"](%3678)\n",
      "  %3680 : __torch__.transformers.modeling_bert.___torch_mangle_1729.BertSelfAttention = prim::GetAttr[name=\"self\"](%3678)\n",
      "  %3681 : __torch__.torch.nn.modules.dropout.___torch_mangle_1728.Dropout = prim::GetAttr[name=\"dropout\"](%3680)\n",
      "  %3682 : __torch__.torch.nn.modules.linear.___torch_mangle_1727.Linear = prim::GetAttr[name=\"value\"](%3680)\n",
      "  %3683 : __torch__.torch.nn.modules.linear.___torch_mangle_1726.Linear = prim::GetAttr[name=\"key\"](%3680)\n",
      "  %3684 : __torch__.torch.nn.modules.linear.___torch_mangle_1725.Linear = prim::GetAttr[name=\"query\"](%3680)\n",
      "  %3685 : Tensor = prim::GetAttr[name=\"bias\"](%3684)\n",
      "  %3686 : Tensor = prim::GetAttr[name=\"weight\"](%3684)\n",
      "  %3687 : Float(768, 768) = aten::t(%3686), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.1 : Float(1, 128, 768) = aten::matmul(%input.5, %3687), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3689 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.1 : Float(1, 128, 768) = aten::add_(%output.1, %3685, %3689), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %3691 : Tensor = prim::GetAttr[name=\"bias\"](%3683)\n",
      "  %3692 : Tensor = prim::GetAttr[name=\"weight\"](%3683)\n",
      "  %3693 : Float(768, 768) = aten::t(%3692), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.2 : Float(1, 128, 768) = aten::matmul(%input.5, %3693), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3695 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.3 : Float(1, 128, 768) = aten::add_(%output.2, %3691, %3695), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %3697 : Tensor = prim::GetAttr[name=\"bias\"](%3682)\n",
      "  %3698 : Tensor = prim::GetAttr[name=\"weight\"](%3682)\n",
      "  %3699 : Float(768, 768) = aten::t(%3698), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.3 : Float(1, 128, 768) = aten::matmul(%input.5, %3699), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3701 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.5 : Float(1, 128, 768) = aten::add_(%output.3, %3697, %3701), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %3703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3704 : int = aten::size(%x.1, %3703), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3705 : Long() = prim::NumToTensor(%3704), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3706 : int = aten::Int(%3705), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3708 : int = aten::size(%x.1, %3707), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3709 : Long() = prim::NumToTensor(%3708), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3710 : int = aten::Int(%3709), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3711 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3712 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3713 : int[] = prim::ListConstruct(%3706, %3710, %3711, %3712), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %x.2 : Float(1, 128, 12, 64) = aten::view(%x.1, %3713), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3715 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3716 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3717 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3718 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3719 : int[] = prim::ListConstruct(%3715, %3716, %3717, %3718), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %query_layer.1 : Float(1, 12, 128, 64) = aten::permute(%x.2, %3719), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3721 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3722 : int = aten::size(%x.3, %3721), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3723 : Long() = prim::NumToTensor(%3722), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3724 : int = aten::Int(%3723), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3725 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3726 : int = aten::size(%x.3, %3725), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3727 : Long() = prim::NumToTensor(%3726), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3728 : int = aten::Int(%3727), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3729 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3730 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3731 : int[] = prim::ListConstruct(%3724, %3728, %3729, %3730), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %x.4 : Float(1, 128, 12, 64) = aten::view(%x.3, %3731), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3733 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3734 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3735 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3736 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3737 : int[] = prim::ListConstruct(%3733, %3734, %3735, %3736), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %key_layer.1 : Float(1, 12, 128, 64) = aten::permute(%x.4, %3737), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3739 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3740 : int = aten::size(%x.5, %3739), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3741 : Long() = prim::NumToTensor(%3740), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3742 : int = aten::Int(%3741), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3743 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3744 : int = aten::size(%x.5, %3743), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3745 : Long() = prim::NumToTensor(%3744), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3746 : int = aten::Int(%3745), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3747 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3748 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3749 : int[] = prim::ListConstruct(%3742, %3746, %3747, %3748), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %x.6 : Float(1, 128, 12, 64) = aten::view(%x.5, %3749), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3751 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3752 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3754 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3755 : int[] = prim::ListConstruct(%3751, %3752, %3753, %3754), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %value_layer.1 : Float(1, 12, 128, 64) = aten::permute(%x.6, %3755), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3757 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %3758 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %3759 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.1, %3757, %3758), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.1 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.1, %3759), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %3761 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.2 : Float(1, 12, 128, 128) = aten::div(%attention_scores.1, %3761), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %3763 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.6 : Float(1, 12, 128, 128) = aten::add(%attention_scores.2, %attention_mask, %3763), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %3765 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %3766 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %input.7 : Float(1, 12, 128, 128) = aten::softmax(%input.6, %3765, %3766), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %3768 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3769 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.1 : Float(1, 12, 128, 128) = aten::dropout(%input.7, %3768, %3769), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.1 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %3772 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3773 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3775 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3776 : int[] = prim::ListConstruct(%3772, %3773, %3774, %3775), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3777 : Float(1, 128, 12, 64) = aten::permute(%context_layer.1, %3776), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3778 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.2 : Float(1, 128, 12, 64) = aten::contiguous(%3777, %3778), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3780 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %3781 : int = aten::size(%context_layer.2, %3780), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %3782 : Long() = prim::NumToTensor(%3781), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3783 : int = aten::Int(%3782), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %3785 : int = aten::size(%context_layer.2, %3784), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %3786 : Long() = prim::NumToTensor(%3785), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3787 : int = aten::Int(%3786), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %3788 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %3789 : int[] = prim::ListConstruct(%3783, %3787, %3788), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
      "  %input.8 : Float(1, 128, 768) = aten::view(%context_layer.2, %3789), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %3791 : __torch__.torch.nn.modules.normalization.___torch_mangle_1731.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%3679)\n",
      "  %3792 : __torch__.torch.nn.modules.dropout.___torch_mangle_1732.Dropout = prim::GetAttr[name=\"dropout\"](%3679)\n",
      "  %3793 : __torch__.torch.nn.modules.linear.___torch_mangle_1730.Linear = prim::GetAttr[name=\"dense\"](%3679)\n",
      "  %3794 : Tensor = prim::GetAttr[name=\"bias\"](%3793)\n",
      "  %3795 : Tensor = prim::GetAttr[name=\"weight\"](%3793)\n",
      "  %3796 : Float(768, 768) = aten::t(%3795), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.4 : Float(1, 128, 768) = aten::matmul(%input.8, %3796), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3798 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.9 : Float(1, 128, 768) = aten::add_(%output.4, %3794, %3798), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %3800 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3801 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.1 : Float(1, 128, 768) = aten::dropout(%input.9, %3800, %3801), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3803 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.10 : Float(1, 128, 768) = aten::add(%hidden_states.1, %input.5, %3803), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %3805 : Tensor = prim::GetAttr[name=\"bias\"](%3791)\n",
      "  %3806 : Tensor = prim::GetAttr[name=\"weight\"](%3791)\n",
      "  %3807 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3808 : int[] = prim::ListConstruct(%3807), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm\n",
      "  %3809 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3810 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.1 : Float(1, 128, 768) = aten::layer_norm(%input.10, %3808, %3806, %3805, %3809, %3810), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3812 : __torch__.torch.nn.modules.linear.___torch_mangle_1735.Linear = prim::GetAttr[name=\"dense\"](%3677)\n",
      "  %3813 : Tensor = prim::GetAttr[name=\"bias\"](%3812)\n",
      "  %3814 : Tensor = prim::GetAttr[name=\"weight\"](%3812)\n",
      "  %3815 : Float(768, 3072) = aten::t(%3814), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.5 : Float(1, 128, 3072) = aten::matmul(%input_tensor.1, %3815), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3817 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.11 : Float(1, 128, 3072) = aten::add_(%output.5, %3813, %3817), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.12 : Float(1, 128, 3072) = aten::gelu(%input.11), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %3820 : __torch__.torch.nn.modules.normalization.___torch_mangle_1738.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%3676)\n",
      "  %3821 : __torch__.torch.nn.modules.dropout.___torch_mangle_1739.Dropout = prim::GetAttr[name=\"dropout\"](%3676)\n",
      "  %3822 : __torch__.torch.nn.modules.linear.___torch_mangle_1737.Linear = prim::GetAttr[name=\"dense\"](%3676)\n",
      "  %3823 : Tensor = prim::GetAttr[name=\"bias\"](%3822)\n",
      "  %3824 : Tensor = prim::GetAttr[name=\"weight\"](%3822)\n",
      "  %3825 : Float(3072, 768) = aten::t(%3824), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.6 : Float(1, 128, 768) = aten::matmul(%input.12, %3825), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3827 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.13 : Float(1, 128, 768) = aten::add_(%output.6, %3823, %3827), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %3829 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3830 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.2 : Float(1, 128, 768) = aten::dropout(%input.13, %3829, %3830), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3832 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.14 : Float(1, 128, 768) = aten::add(%hidden_states.2, %input_tensor.1, %3832), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %3834 : Tensor = prim::GetAttr[name=\"bias\"](%3820)\n",
      "  %3835 : Tensor = prim::GetAttr[name=\"weight\"](%3820)\n",
      "  %3836 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3837 : int[] = prim::ListConstruct(%3836), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm\n",
      "  %3838 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3839 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.15 : Float(1, 128, 768) = aten::layer_norm(%input.14, %3837, %3835, %3834, %3838, %3839), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3841 : __torch__.transformers.modeling_bert.___torch_mangle_1757.BertOutput = prim::GetAttr[name=\"output\"](%3673)\n",
      "  %3842 : __torch__.transformers.modeling_bert.___torch_mangle_1753.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3673)\n",
      "  %3843 : __torch__.transformers.modeling_bert.___torch_mangle_1751.BertAttention = prim::GetAttr[name=\"attention\"](%3673)\n",
      "  %3844 : __torch__.transformers.modeling_bert.___torch_mangle_1750.BertSelfOutput = prim::GetAttr[name=\"output\"](%3843)\n",
      "  %3845 : __torch__.transformers.modeling_bert.___torch_mangle_1746.BertSelfAttention = prim::GetAttr[name=\"self\"](%3843)\n",
      "  %3846 : __torch__.torch.nn.modules.dropout.___torch_mangle_1745.Dropout = prim::GetAttr[name=\"dropout\"](%3845)\n",
      "  %3847 : __torch__.torch.nn.modules.linear.___torch_mangle_1744.Linear = prim::GetAttr[name=\"value\"](%3845)\n",
      "  %3848 : __torch__.torch.nn.modules.linear.___torch_mangle_1743.Linear = prim::GetAttr[name=\"key\"](%3845)\n",
      "  %3849 : __torch__.torch.nn.modules.linear.___torch_mangle_1742.Linear = prim::GetAttr[name=\"query\"](%3845)\n",
      "  %3850 : Tensor = prim::GetAttr[name=\"bias\"](%3849)\n",
      "  %3851 : Tensor = prim::GetAttr[name=\"weight\"](%3849)\n",
      "  %3852 : Float(768, 768) = aten::t(%3851), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.7 : Float(1, 128, 768) = aten::matmul(%input.15, %3852), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.7 : Float(1, 128, 768) = aten::add_(%output.7, %3850, %3854), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %3856 : Tensor = prim::GetAttr[name=\"bias\"](%3848)\n",
      "  %3857 : Tensor = prim::GetAttr[name=\"weight\"](%3848)\n",
      "  %3858 : Float(768, 768) = aten::t(%3857), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.8 : Float(1, 128, 768) = aten::matmul(%input.15, %3858), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3860 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.9 : Float(1, 128, 768) = aten::add_(%output.8, %3856, %3860), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %3862 : Tensor = prim::GetAttr[name=\"bias\"](%3847)\n",
      "  %3863 : Tensor = prim::GetAttr[name=\"weight\"](%3847)\n",
      "  %3864 : Float(768, 768) = aten::t(%3863), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.9 : Float(1, 128, 768) = aten::matmul(%input.15, %3864), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3866 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.11 : Float(1, 128, 768) = aten::add_(%output.9, %3862, %3866), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %3868 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3869 : int = aten::size(%x.7, %3868), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3870 : Long() = prim::NumToTensor(%3869), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3871 : int = aten::Int(%3870), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3872 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3873 : int = aten::size(%x.7, %3872), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3874 : Long() = prim::NumToTensor(%3873), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3875 : int = aten::Int(%3874), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3876 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3877 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3878 : int[] = prim::ListConstruct(%3871, %3875, %3876, %3877), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %x.8 : Float(1, 128, 12, 64) = aten::view(%x.7, %3878), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3880 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3881 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3883 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3884 : int[] = prim::ListConstruct(%3880, %3881, %3882, %3883), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %query_layer.2 : Float(1, 12, 128, 64) = aten::permute(%x.8, %3884), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3886 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3887 : int = aten::size(%x.9, %3886), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3888 : Long() = prim::NumToTensor(%3887), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3889 : int = aten::Int(%3888), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3891 : int = aten::size(%x.9, %3890), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3892 : Long() = prim::NumToTensor(%3891), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3893 : int = aten::Int(%3892), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3894 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3895 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3896 : int[] = prim::ListConstruct(%3889, %3893, %3894, %3895), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %x.10 : Float(1, 128, 12, 64) = aten::view(%x.9, %3896), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3898 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3899 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3901 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3902 : int[] = prim::ListConstruct(%3898, %3899, %3900, %3901), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %key_layer.2 : Float(1, 12, 128, 64) = aten::permute(%x.10, %3902), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3904 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3905 : int = aten::size(%x.11, %3904), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3906 : Long() = prim::NumToTensor(%3905), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3907 : int = aten::Int(%3906), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3909 : int = aten::size(%x.11, %3908), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %3910 : Long() = prim::NumToTensor(%3909), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3911 : int = aten::Int(%3910), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3912 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3913 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3914 : int[] = prim::ListConstruct(%3907, %3911, %3912, %3913), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %x.12 : Float(1, 128, 12, 64) = aten::view(%x.11, %3914), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %3916 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3917 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3918 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3919 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3920 : int[] = prim::ListConstruct(%3916, %3917, %3918, %3919), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %value_layer.2 : Float(1, 12, 128, 64) = aten::permute(%x.12, %3920), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %3922 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %3923 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %3924 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.2, %3922, %3923), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.3 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.2, %3924), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %3926 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.4 : Float(1, 12, 128, 128) = aten::div(%attention_scores.3, %3926), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %3928 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.16 : Float(1, 12, 128, 128) = aten::add(%attention_scores.4, %attention_mask, %3928), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %3930 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %3931 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %input.17 : Float(1, 12, 128, 128) = aten::softmax(%input.16, %3930, %3931), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %3933 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3934 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.2 : Float(1, 12, 128, 128) = aten::dropout(%input.17, %3933, %3934), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.3 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %3937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3938 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3939 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3940 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3941 : int[] = prim::ListConstruct(%3937, %3938, %3939, %3940), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3942 : Float(1, 128, 12, 64) = aten::permute(%context_layer.3, %3941), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3943 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.4 : Float(1, 128, 12, 64) = aten::contiguous(%3942, %3943), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %3945 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %3946 : int = aten::size(%context_layer.4, %3945), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %3947 : Long() = prim::NumToTensor(%3946), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3948 : int = aten::Int(%3947), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3949 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %3950 : int = aten::size(%context_layer.4, %3949), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %3951 : Long() = prim::NumToTensor(%3950), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3952 : int = aten::Int(%3951), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %3953 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %3954 : int[] = prim::ListConstruct(%3948, %3952, %3953), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
      "  %input.18 : Float(1, 128, 768) = aten::view(%context_layer.4, %3954), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %3956 : __torch__.torch.nn.modules.normalization.___torch_mangle_1748.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%3844)\n",
      "  %3957 : __torch__.torch.nn.modules.dropout.___torch_mangle_1749.Dropout = prim::GetAttr[name=\"dropout\"](%3844)\n",
      "  %3958 : __torch__.torch.nn.modules.linear.___torch_mangle_1747.Linear = prim::GetAttr[name=\"dense\"](%3844)\n",
      "  %3959 : Tensor = prim::GetAttr[name=\"bias\"](%3958)\n",
      "  %3960 : Tensor = prim::GetAttr[name=\"weight\"](%3958)\n",
      "  %3961 : Float(768, 768) = aten::t(%3960), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.10 : Float(1, 128, 768) = aten::matmul(%input.18, %3961), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3963 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.19 : Float(1, 128, 768) = aten::add_(%output.10, %3959, %3963), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %3965 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3966 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.3 : Float(1, 128, 768) = aten::dropout(%input.19, %3965, %3966), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.20 : Float(1, 128, 768) = aten::add(%hidden_states.3, %input.15, %3968), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %3970 : Tensor = prim::GetAttr[name=\"bias\"](%3956)\n",
      "  %3971 : Tensor = prim::GetAttr[name=\"weight\"](%3956)\n",
      "  %3972 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3973 : int[] = prim::ListConstruct(%3972), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm\n",
      "  %3974 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3975 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.2 : Float(1, 128, 768) = aten::layer_norm(%input.20, %3973, %3971, %3970, %3974, %3975), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %3977 : __torch__.torch.nn.modules.linear.___torch_mangle_1752.Linear = prim::GetAttr[name=\"dense\"](%3842)\n",
      "  %3978 : Tensor = prim::GetAttr[name=\"bias\"](%3977)\n",
      "  %3979 : Tensor = prim::GetAttr[name=\"weight\"](%3977)\n",
      "  %3980 : Float(768, 3072) = aten::t(%3979), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.11 : Float(1, 128, 3072) = aten::matmul(%input_tensor.2, %3980), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3982 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.21 : Float(1, 128, 3072) = aten::add_(%output.11, %3978, %3982), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.22 : Float(1, 128, 3072) = aten::gelu(%input.21), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %3985 : __torch__.torch.nn.modules.normalization.___torch_mangle_1755.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%3841)\n",
      "  %3986 : __torch__.torch.nn.modules.dropout.___torch_mangle_1756.Dropout = prim::GetAttr[name=\"dropout\"](%3841)\n",
      "  %3987 : __torch__.torch.nn.modules.linear.___torch_mangle_1754.Linear = prim::GetAttr[name=\"dense\"](%3841)\n",
      "  %3988 : Tensor = prim::GetAttr[name=\"bias\"](%3987)\n",
      "  %3989 : Tensor = prim::GetAttr[name=\"weight\"](%3987)\n",
      "  %3990 : Float(3072, 768) = aten::t(%3989), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.12 : Float(1, 128, 768) = aten::matmul(%input.22, %3990), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %3992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.23 : Float(1, 128, 768) = aten::add_(%output.12, %3988, %3992), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %3994 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3995 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.4 : Float(1, 128, 768) = aten::dropout(%input.23, %3994, %3995), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %3997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.24 : Float(1, 128, 768) = aten::add(%hidden_states.4, %input_tensor.2, %3997), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %3999 : Tensor = prim::GetAttr[name=\"bias\"](%3985)\n",
      "  %4000 : Tensor = prim::GetAttr[name=\"weight\"](%3985)\n",
      "  %4001 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4002 : int[] = prim::ListConstruct(%4001), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm\n",
      "  %4003 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4004 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.25 : Float(1, 128, 768) = aten::layer_norm(%input.24, %4002, %4000, %3999, %4003, %4004), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4006 : __torch__.transformers.modeling_bert.___torch_mangle_1774.BertOutput = prim::GetAttr[name=\"output\"](%3671)\n",
      "  %4007 : __torch__.transformers.modeling_bert.___torch_mangle_1770.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3671)\n",
      "  %4008 : __torch__.transformers.modeling_bert.___torch_mangle_1768.BertAttention = prim::GetAttr[name=\"attention\"](%3671)\n",
      "  %4009 : __torch__.transformers.modeling_bert.___torch_mangle_1767.BertSelfOutput = prim::GetAttr[name=\"output\"](%4008)\n",
      "  %4010 : __torch__.transformers.modeling_bert.___torch_mangle_1763.BertSelfAttention = prim::GetAttr[name=\"self\"](%4008)\n",
      "  %4011 : __torch__.torch.nn.modules.dropout.___torch_mangle_1762.Dropout = prim::GetAttr[name=\"dropout\"](%4010)\n",
      "  %4012 : __torch__.torch.nn.modules.linear.___torch_mangle_1761.Linear = prim::GetAttr[name=\"value\"](%4010)\n",
      "  %4013 : __torch__.torch.nn.modules.linear.___torch_mangle_1760.Linear = prim::GetAttr[name=\"key\"](%4010)\n",
      "  %4014 : __torch__.torch.nn.modules.linear.___torch_mangle_1759.Linear = prim::GetAttr[name=\"query\"](%4010)\n",
      "  %4015 : Tensor = prim::GetAttr[name=\"bias\"](%4014)\n",
      "  %4016 : Tensor = prim::GetAttr[name=\"weight\"](%4014)\n",
      "  %4017 : Float(768, 768) = aten::t(%4016), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.13 : Float(1, 128, 768) = aten::matmul(%input.25, %4017), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4019 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.13 : Float(1, 128, 768) = aten::add_(%output.13, %4015, %4019), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4021 : Tensor = prim::GetAttr[name=\"bias\"](%4013)\n",
      "  %4022 : Tensor = prim::GetAttr[name=\"weight\"](%4013)\n",
      "  %4023 : Float(768, 768) = aten::t(%4022), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.14 : Float(1, 128, 768) = aten::matmul(%input.25, %4023), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4025 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.15 : Float(1, 128, 768) = aten::add_(%output.14, %4021, %4025), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4027 : Tensor = prim::GetAttr[name=\"bias\"](%4012)\n",
      "  %4028 : Tensor = prim::GetAttr[name=\"weight\"](%4012)\n",
      "  %4029 : Float(768, 768) = aten::t(%4028), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.15 : Float(1, 128, 768) = aten::matmul(%input.25, %4029), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4031 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.17 : Float(1, 128, 768) = aten::add_(%output.15, %4027, %4031), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4033 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4034 : int = aten::size(%x.13, %4033), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4035 : Long() = prim::NumToTensor(%4034), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4036 : int = aten::Int(%4035), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4037 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4038 : int = aten::size(%x.13, %4037), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4039 : Long() = prim::NumToTensor(%4038), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4040 : int = aten::Int(%4039), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4041 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4042 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4043 : int[] = prim::ListConstruct(%4036, %4040, %4041, %4042), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %x.14 : Float(1, 128, 12, 64) = aten::view(%x.13, %4043), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4046 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4047 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4048 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4049 : int[] = prim::ListConstruct(%4045, %4046, %4047, %4048), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %query_layer.3 : Float(1, 12, 128, 64) = aten::permute(%x.14, %4049), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4052 : int = aten::size(%x.15, %4051), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4053 : Long() = prim::NumToTensor(%4052), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4054 : int = aten::Int(%4053), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4055 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4056 : int = aten::size(%x.15, %4055), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4057 : Long() = prim::NumToTensor(%4056), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4058 : int = aten::Int(%4057), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4059 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4060 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4061 : int[] = prim::ListConstruct(%4054, %4058, %4059, %4060), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %x.16 : Float(1, 128, 12, 64) = aten::view(%x.15, %4061), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4063 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4064 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4065 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4066 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4067 : int[] = prim::ListConstruct(%4063, %4064, %4065, %4066), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %key_layer.3 : Float(1, 12, 128, 64) = aten::permute(%x.16, %4067), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4069 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4070 : int = aten::size(%x.17, %4069), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4071 : Long() = prim::NumToTensor(%4070), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4072 : int = aten::Int(%4071), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4073 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4074 : int = aten::size(%x.17, %4073), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4075 : Long() = prim::NumToTensor(%4074), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4076 : int = aten::Int(%4075), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4077 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4078 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4079 : int[] = prim::ListConstruct(%4072, %4076, %4077, %4078), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %x.18 : Float(1, 128, 12, 64) = aten::view(%x.17, %4079), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4081 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4082 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4083 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4084 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4085 : int[] = prim::ListConstruct(%4081, %4082, %4083, %4084), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %value_layer.3 : Float(1, 12, 128, 64) = aten::permute(%x.18, %4085), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4087 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4088 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4089 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.3, %4087, %4088), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.5 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.3, %4089), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4091 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.6 : Float(1, 12, 128, 128) = aten::div(%attention_scores.5, %4091), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %4093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.26 : Float(1, 12, 128, 128) = aten::add(%attention_scores.6, %attention_mask, %4093), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %4095 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4096 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %input.27 : Float(1, 12, 128, 128) = aten::softmax(%input.26, %4095, %4096), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4098 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4099 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.3 : Float(1, 12, 128, 128) = aten::dropout(%input.27, %4098, %4099), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.5 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %4102 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4103 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4104 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4105 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4106 : int[] = prim::ListConstruct(%4102, %4103, %4104, %4105), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4107 : Float(1, 128, 12, 64) = aten::permute(%context_layer.5, %4106), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.6 : Float(1, 128, 12, 64) = aten::contiguous(%4107, %4108), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4110 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4111 : int = aten::size(%context_layer.6, %4110), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4112 : Long() = prim::NumToTensor(%4111), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4113 : int = aten::Int(%4112), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4115 : int = aten::size(%context_layer.6, %4114), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4116 : Long() = prim::NumToTensor(%4115), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4117 : int = aten::Int(%4116), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %4118 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4119 : int[] = prim::ListConstruct(%4113, %4117, %4118), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self\n",
      "  %input.28 : Float(1, 128, 768) = aten::view(%context_layer.6, %4119), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4121 : __torch__.torch.nn.modules.normalization.___torch_mangle_1765.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4009)\n",
      "  %4122 : __torch__.torch.nn.modules.dropout.___torch_mangle_1766.Dropout = prim::GetAttr[name=\"dropout\"](%4009)\n",
      "  %4123 : __torch__.torch.nn.modules.linear.___torch_mangle_1764.Linear = prim::GetAttr[name=\"dense\"](%4009)\n",
      "  %4124 : Tensor = prim::GetAttr[name=\"bias\"](%4123)\n",
      "  %4125 : Tensor = prim::GetAttr[name=\"weight\"](%4123)\n",
      "  %4126 : Float(768, 768) = aten::t(%4125), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.16 : Float(1, 128, 768) = aten::matmul(%input.28, %4126), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4128 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.29 : Float(1, 128, 768) = aten::add_(%output.16, %4124, %4128), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4130 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4131 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.5 : Float(1, 128, 768) = aten::dropout(%input.29, %4130, %4131), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4133 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.30 : Float(1, 128, 768) = aten::add(%hidden_states.5, %input.25, %4133), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %4135 : Tensor = prim::GetAttr[name=\"bias\"](%4121)\n",
      "  %4136 : Tensor = prim::GetAttr[name=\"weight\"](%4121)\n",
      "  %4137 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4138 : int[] = prim::ListConstruct(%4137), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm\n",
      "  %4139 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4140 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.3 : Float(1, 128, 768) = aten::layer_norm(%input.30, %4138, %4136, %4135, %4139, %4140), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4142 : __torch__.torch.nn.modules.linear.___torch_mangle_1769.Linear = prim::GetAttr[name=\"dense\"](%4007)\n",
      "  %4143 : Tensor = prim::GetAttr[name=\"bias\"](%4142)\n",
      "  %4144 : Tensor = prim::GetAttr[name=\"weight\"](%4142)\n",
      "  %4145 : Float(768, 3072) = aten::t(%4144), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.17 : Float(1, 128, 3072) = aten::matmul(%input_tensor.3, %4145), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4147 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.31 : Float(1, 128, 3072) = aten::add_(%output.17, %4143, %4147), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.32 : Float(1, 128, 3072) = aten::gelu(%input.31), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %4150 : __torch__.torch.nn.modules.normalization.___torch_mangle_1772.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4006)\n",
      "  %4151 : __torch__.torch.nn.modules.dropout.___torch_mangle_1773.Dropout = prim::GetAttr[name=\"dropout\"](%4006)\n",
      "  %4152 : __torch__.torch.nn.modules.linear.___torch_mangle_1771.Linear = prim::GetAttr[name=\"dense\"](%4006)\n",
      "  %4153 : Tensor = prim::GetAttr[name=\"bias\"](%4152)\n",
      "  %4154 : Tensor = prim::GetAttr[name=\"weight\"](%4152)\n",
      "  %4155 : Float(3072, 768) = aten::t(%4154), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.18 : Float(1, 128, 768) = aten::matmul(%input.32, %4155), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4157 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.33 : Float(1, 128, 768) = aten::add_(%output.18, %4153, %4157), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4159 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4160 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.6 : Float(1, 128, 768) = aten::dropout(%input.33, %4159, %4160), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4162 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.34 : Float(1, 128, 768) = aten::add(%hidden_states.6, %input_tensor.3, %4162), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %4164 : Tensor = prim::GetAttr[name=\"bias\"](%4150)\n",
      "  %4165 : Tensor = prim::GetAttr[name=\"weight\"](%4150)\n",
      "  %4166 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4167 : int[] = prim::ListConstruct(%4166), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm\n",
      "  %4168 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4169 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.35 : Float(1, 128, 768) = aten::layer_norm(%input.34, %4167, %4165, %4164, %4168, %4169), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4171 : __torch__.transformers.modeling_bert.___torch_mangle_1791.BertOutput = prim::GetAttr[name=\"output\"](%3669)\n",
      "  %4172 : __torch__.transformers.modeling_bert.___torch_mangle_1787.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3669)\n",
      "  %4173 : __torch__.transformers.modeling_bert.___torch_mangle_1785.BertAttention = prim::GetAttr[name=\"attention\"](%3669)\n",
      "  %4174 : __torch__.transformers.modeling_bert.___torch_mangle_1784.BertSelfOutput = prim::GetAttr[name=\"output\"](%4173)\n",
      "  %4175 : __torch__.transformers.modeling_bert.___torch_mangle_1780.BertSelfAttention = prim::GetAttr[name=\"self\"](%4173)\n",
      "  %4176 : __torch__.torch.nn.modules.dropout.___torch_mangle_1779.Dropout = prim::GetAttr[name=\"dropout\"](%4175)\n",
      "  %4177 : __torch__.torch.nn.modules.linear.___torch_mangle_1778.Linear = prim::GetAttr[name=\"value\"](%4175)\n",
      "  %4178 : __torch__.torch.nn.modules.linear.___torch_mangle_1777.Linear = prim::GetAttr[name=\"key\"](%4175)\n",
      "  %4179 : __torch__.torch.nn.modules.linear.___torch_mangle_1776.Linear = prim::GetAttr[name=\"query\"](%4175)\n",
      "  %4180 : Tensor = prim::GetAttr[name=\"bias\"](%4179)\n",
      "  %4181 : Tensor = prim::GetAttr[name=\"weight\"](%4179)\n",
      "  %4182 : Float(768, 768) = aten::t(%4181), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.19 : Float(1, 128, 768) = aten::matmul(%input.35, %4182), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4184 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.19 : Float(1, 128, 768) = aten::add_(%output.19, %4180, %4184), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4186 : Tensor = prim::GetAttr[name=\"bias\"](%4178)\n",
      "  %4187 : Tensor = prim::GetAttr[name=\"weight\"](%4178)\n",
      "  %4188 : Float(768, 768) = aten::t(%4187), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.20 : Float(1, 128, 768) = aten::matmul(%input.35, %4188), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4190 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.21 : Float(1, 128, 768) = aten::add_(%output.20, %4186, %4190), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4192 : Tensor = prim::GetAttr[name=\"bias\"](%4177)\n",
      "  %4193 : Tensor = prim::GetAttr[name=\"weight\"](%4177)\n",
      "  %4194 : Float(768, 768) = aten::t(%4193), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.21 : Float(1, 128, 768) = aten::matmul(%input.35, %4194), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4196 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.23 : Float(1, 128, 768) = aten::add_(%output.21, %4192, %4196), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4198 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4199 : int = aten::size(%x.19, %4198), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4200 : Long() = prim::NumToTensor(%4199), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4201 : int = aten::Int(%4200), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4202 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4203 : int = aten::size(%x.19, %4202), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4204 : Long() = prim::NumToTensor(%4203), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4205 : int = aten::Int(%4204), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4206 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4207 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4208 : int[] = prim::ListConstruct(%4201, %4205, %4206, %4207), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %x.20 : Float(1, 128, 12, 64) = aten::view(%x.19, %4208), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4210 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4211 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4212 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4213 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4214 : int[] = prim::ListConstruct(%4210, %4211, %4212, %4213), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %query_layer.4 : Float(1, 12, 128, 64) = aten::permute(%x.20, %4214), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4216 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4217 : int = aten::size(%x.21, %4216), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4218 : Long() = prim::NumToTensor(%4217), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4219 : int = aten::Int(%4218), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4220 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4221 : int = aten::size(%x.21, %4220), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4222 : Long() = prim::NumToTensor(%4221), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4223 : int = aten::Int(%4222), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4224 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4225 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4226 : int[] = prim::ListConstruct(%4219, %4223, %4224, %4225), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %x.22 : Float(1, 128, 12, 64) = aten::view(%x.21, %4226), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4228 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4229 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4230 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4231 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4232 : int[] = prim::ListConstruct(%4228, %4229, %4230, %4231), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %key_layer.4 : Float(1, 12, 128, 64) = aten::permute(%x.22, %4232), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4234 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4235 : int = aten::size(%x.23, %4234), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4236 : Long() = prim::NumToTensor(%4235), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4237 : int = aten::Int(%4236), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4238 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4239 : int = aten::size(%x.23, %4238), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4240 : Long() = prim::NumToTensor(%4239), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4241 : int = aten::Int(%4240), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4242 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4243 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4244 : int[] = prim::ListConstruct(%4237, %4241, %4242, %4243), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %x.24 : Float(1, 128, 12, 64) = aten::view(%x.23, %4244), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4246 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4247 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4248 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4249 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4250 : int[] = prim::ListConstruct(%4246, %4247, %4248, %4249), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %value_layer.4 : Float(1, 12, 128, 64) = aten::permute(%x.24, %4250), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4252 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4253 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4254 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.4, %4252, %4253), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.7 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.4, %4254), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4256 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.8 : Float(1, 12, 128, 128) = aten::div(%attention_scores.7, %4256), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %4258 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.36 : Float(1, 12, 128, 128) = aten::add(%attention_scores.8, %attention_mask, %4258), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %4260 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4261 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %input.37 : Float(1, 12, 128, 128) = aten::softmax(%input.36, %4260, %4261), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4263 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4264 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.4 : Float(1, 12, 128, 128) = aten::dropout(%input.37, %4263, %4264), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.7 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %4267 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4268 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4269 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4270 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4271 : int[] = prim::ListConstruct(%4267, %4268, %4269, %4270), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4272 : Float(1, 128, 12, 64) = aten::permute(%context_layer.7, %4271), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4273 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.8 : Float(1, 128, 12, 64) = aten::contiguous(%4272, %4273), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4275 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4276 : int = aten::size(%context_layer.8, %4275), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4277 : Long() = prim::NumToTensor(%4276), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4278 : int = aten::Int(%4277), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4279 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4280 : int = aten::size(%context_layer.8, %4279), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4281 : Long() = prim::NumToTensor(%4280), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4282 : int = aten::Int(%4281), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %4283 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4284 : int[] = prim::ListConstruct(%4278, %4282, %4283), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self\n",
      "  %input.38 : Float(1, 128, 768) = aten::view(%context_layer.8, %4284), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4286 : __torch__.torch.nn.modules.normalization.___torch_mangle_1782.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4174)\n",
      "  %4287 : __torch__.torch.nn.modules.dropout.___torch_mangle_1783.Dropout = prim::GetAttr[name=\"dropout\"](%4174)\n",
      "  %4288 : __torch__.torch.nn.modules.linear.___torch_mangle_1781.Linear = prim::GetAttr[name=\"dense\"](%4174)\n",
      "  %4289 : Tensor = prim::GetAttr[name=\"bias\"](%4288)\n",
      "  %4290 : Tensor = prim::GetAttr[name=\"weight\"](%4288)\n",
      "  %4291 : Float(768, 768) = aten::t(%4290), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.22 : Float(1, 128, 768) = aten::matmul(%input.38, %4291), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.39 : Float(1, 128, 768) = aten::add_(%output.22, %4289, %4293), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4295 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4296 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.7 : Float(1, 128, 768) = aten::dropout(%input.39, %4295, %4296), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.40 : Float(1, 128, 768) = aten::add(%hidden_states.7, %input.35, %4298), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %4300 : Tensor = prim::GetAttr[name=\"bias\"](%4286)\n",
      "  %4301 : Tensor = prim::GetAttr[name=\"weight\"](%4286)\n",
      "  %4302 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4303 : int[] = prim::ListConstruct(%4302), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm\n",
      "  %4304 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4305 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.4 : Float(1, 128, 768) = aten::layer_norm(%input.40, %4303, %4301, %4300, %4304, %4305), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4307 : __torch__.torch.nn.modules.linear.___torch_mangle_1786.Linear = prim::GetAttr[name=\"dense\"](%4172)\n",
      "  %4308 : Tensor = prim::GetAttr[name=\"bias\"](%4307)\n",
      "  %4309 : Tensor = prim::GetAttr[name=\"weight\"](%4307)\n",
      "  %4310 : Float(768, 3072) = aten::t(%4309), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.23 : Float(1, 128, 3072) = aten::matmul(%input_tensor.4, %4310), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4312 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.41 : Float(1, 128, 3072) = aten::add_(%output.23, %4308, %4312), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.42 : Float(1, 128, 3072) = aten::gelu(%input.41), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %4315 : __torch__.torch.nn.modules.normalization.___torch_mangle_1789.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4171)\n",
      "  %4316 : __torch__.torch.nn.modules.dropout.___torch_mangle_1790.Dropout = prim::GetAttr[name=\"dropout\"](%4171)\n",
      "  %4317 : __torch__.torch.nn.modules.linear.___torch_mangle_1788.Linear = prim::GetAttr[name=\"dense\"](%4171)\n",
      "  %4318 : Tensor = prim::GetAttr[name=\"bias\"](%4317)\n",
      "  %4319 : Tensor = prim::GetAttr[name=\"weight\"](%4317)\n",
      "  %4320 : Float(3072, 768) = aten::t(%4319), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.24 : Float(1, 128, 768) = aten::matmul(%input.42, %4320), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4322 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.43 : Float(1, 128, 768) = aten::add_(%output.24, %4318, %4322), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4324 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4325 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.8 : Float(1, 128, 768) = aten::dropout(%input.43, %4324, %4325), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4327 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.44 : Float(1, 128, 768) = aten::add(%hidden_states.8, %input_tensor.4, %4327), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %4329 : Tensor = prim::GetAttr[name=\"bias\"](%4315)\n",
      "  %4330 : Tensor = prim::GetAttr[name=\"weight\"](%4315)\n",
      "  %4331 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4332 : int[] = prim::ListConstruct(%4331), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm\n",
      "  %4333 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4334 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.45 : Float(1, 128, 768) = aten::layer_norm(%input.44, %4332, %4330, %4329, %4333, %4334), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4336 : __torch__.transformers.modeling_bert.___torch_mangle_1808.BertOutput = prim::GetAttr[name=\"output\"](%3667)\n",
      "  %4337 : __torch__.transformers.modeling_bert.___torch_mangle_1804.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3667)\n",
      "  %4338 : __torch__.transformers.modeling_bert.___torch_mangle_1802.BertAttention = prim::GetAttr[name=\"attention\"](%3667)\n",
      "  %4339 : __torch__.transformers.modeling_bert.___torch_mangle_1801.BertSelfOutput = prim::GetAttr[name=\"output\"](%4338)\n",
      "  %4340 : __torch__.transformers.modeling_bert.___torch_mangle_1797.BertSelfAttention = prim::GetAttr[name=\"self\"](%4338)\n",
      "  %4341 : __torch__.torch.nn.modules.dropout.___torch_mangle_1796.Dropout = prim::GetAttr[name=\"dropout\"](%4340)\n",
      "  %4342 : __torch__.torch.nn.modules.linear.___torch_mangle_1795.Linear = prim::GetAttr[name=\"value\"](%4340)\n",
      "  %4343 : __torch__.torch.nn.modules.linear.___torch_mangle_1794.Linear = prim::GetAttr[name=\"key\"](%4340)\n",
      "  %4344 : __torch__.torch.nn.modules.linear.___torch_mangle_1793.Linear = prim::GetAttr[name=\"query\"](%4340)\n",
      "  %4345 : Tensor = prim::GetAttr[name=\"bias\"](%4344)\n",
      "  %4346 : Tensor = prim::GetAttr[name=\"weight\"](%4344)\n",
      "  %4347 : Float(768, 768) = aten::t(%4346), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.25 : Float(1, 128, 768) = aten::matmul(%input.45, %4347), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.25 : Float(1, 128, 768) = aten::add_(%output.25, %4345, %4349), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4351 : Tensor = prim::GetAttr[name=\"bias\"](%4343)\n",
      "  %4352 : Tensor = prim::GetAttr[name=\"weight\"](%4343)\n",
      "  %4353 : Float(768, 768) = aten::t(%4352), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.26 : Float(1, 128, 768) = aten::matmul(%input.45, %4353), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4355 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.27 : Float(1, 128, 768) = aten::add_(%output.26, %4351, %4355), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4357 : Tensor = prim::GetAttr[name=\"bias\"](%4342)\n",
      "  %4358 : Tensor = prim::GetAttr[name=\"weight\"](%4342)\n",
      "  %4359 : Float(768, 768) = aten::t(%4358), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.27 : Float(1, 128, 768) = aten::matmul(%input.45, %4359), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4361 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.29 : Float(1, 128, 768) = aten::add_(%output.27, %4357, %4361), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4363 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4364 : int = aten::size(%x.25, %4363), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4365 : Long() = prim::NumToTensor(%4364), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4366 : int = aten::Int(%4365), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4367 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4368 : int = aten::size(%x.25, %4367), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4369 : Long() = prim::NumToTensor(%4368), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4370 : int = aten::Int(%4369), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4371 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4372 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4373 : int[] = prim::ListConstruct(%4366, %4370, %4371, %4372), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %x.26 : Float(1, 128, 12, 64) = aten::view(%x.25, %4373), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4375 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4376 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4377 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4378 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4379 : int[] = prim::ListConstruct(%4375, %4376, %4377, %4378), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %query_layer.5 : Float(1, 12, 128, 64) = aten::permute(%x.26, %4379), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4382 : int = aten::size(%x.27, %4381), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4383 : Long() = prim::NumToTensor(%4382), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4384 : int = aten::Int(%4383), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4386 : int = aten::size(%x.27, %4385), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4387 : Long() = prim::NumToTensor(%4386), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4388 : int = aten::Int(%4387), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4389 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4390 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4391 : int[] = prim::ListConstruct(%4384, %4388, %4389, %4390), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %x.28 : Float(1, 128, 12, 64) = aten::view(%x.27, %4391), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4393 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4394 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4395 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4397 : int[] = prim::ListConstruct(%4393, %4394, %4395, %4396), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %key_layer.5 : Float(1, 12, 128, 64) = aten::permute(%x.28, %4397), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4399 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4400 : int = aten::size(%x.29, %4399), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4401 : Long() = prim::NumToTensor(%4400), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4402 : int = aten::Int(%4401), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4403 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4404 : int = aten::size(%x.29, %4403), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4405 : Long() = prim::NumToTensor(%4404), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4406 : int = aten::Int(%4405), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4407 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4408 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4409 : int[] = prim::ListConstruct(%4402, %4406, %4407, %4408), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %x.30 : Float(1, 128, 12, 64) = aten::view(%x.29, %4409), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4411 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4412 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4413 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4414 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4415 : int[] = prim::ListConstruct(%4411, %4412, %4413, %4414), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %value_layer.5 : Float(1, 12, 128, 64) = aten::permute(%x.30, %4415), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4417 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4418 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4419 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.5, %4417, %4418), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.9 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.5, %4419), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4421 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.10 : Float(1, 12, 128, 128) = aten::div(%attention_scores.9, %4421), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %4423 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.46 : Float(1, 12, 128, 128) = aten::add(%attention_scores.10, %attention_mask, %4423), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %4425 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4426 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %input.47 : Float(1, 12, 128, 128) = aten::softmax(%input.46, %4425, %4426), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4428 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4429 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.5 : Float(1, 12, 128, 128) = aten::dropout(%input.47, %4428, %4429), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.9 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %4432 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4433 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4434 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4435 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4436 : int[] = prim::ListConstruct(%4432, %4433, %4434, %4435), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4437 : Float(1, 128, 12, 64) = aten::permute(%context_layer.9, %4436), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4438 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.10 : Float(1, 128, 12, 64) = aten::contiguous(%4437, %4438), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4440 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4441 : int = aten::size(%context_layer.10, %4440), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4442 : Long() = prim::NumToTensor(%4441), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4443 : int = aten::Int(%4442), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4444 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4445 : int = aten::size(%context_layer.10, %4444), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4446 : Long() = prim::NumToTensor(%4445), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4447 : int = aten::Int(%4446), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %4448 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4449 : int[] = prim::ListConstruct(%4443, %4447, %4448), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self\n",
      "  %input.48 : Float(1, 128, 768) = aten::view(%context_layer.10, %4449), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4451 : __torch__.torch.nn.modules.normalization.___torch_mangle_1799.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4339)\n",
      "  %4452 : __torch__.torch.nn.modules.dropout.___torch_mangle_1800.Dropout = prim::GetAttr[name=\"dropout\"](%4339)\n",
      "  %4453 : __torch__.torch.nn.modules.linear.___torch_mangle_1798.Linear = prim::GetAttr[name=\"dense\"](%4339)\n",
      "  %4454 : Tensor = prim::GetAttr[name=\"bias\"](%4453)\n",
      "  %4455 : Tensor = prim::GetAttr[name=\"weight\"](%4453)\n",
      "  %4456 : Float(768, 768) = aten::t(%4455), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.28 : Float(1, 128, 768) = aten::matmul(%input.48, %4456), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4458 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.49 : Float(1, 128, 768) = aten::add_(%output.28, %4454, %4458), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4460 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4461 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.9 : Float(1, 128, 768) = aten::dropout(%input.49, %4460, %4461), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4463 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.50 : Float(1, 128, 768) = aten::add(%hidden_states.9, %input.45, %4463), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %4465 : Tensor = prim::GetAttr[name=\"bias\"](%4451)\n",
      "  %4466 : Tensor = prim::GetAttr[name=\"weight\"](%4451)\n",
      "  %4467 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4468 : int[] = prim::ListConstruct(%4467), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm\n",
      "  %4469 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4470 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.5 : Float(1, 128, 768) = aten::layer_norm(%input.50, %4468, %4466, %4465, %4469, %4470), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4472 : __torch__.torch.nn.modules.linear.___torch_mangle_1803.Linear = prim::GetAttr[name=\"dense\"](%4337)\n",
      "  %4473 : Tensor = prim::GetAttr[name=\"bias\"](%4472)\n",
      "  %4474 : Tensor = prim::GetAttr[name=\"weight\"](%4472)\n",
      "  %4475 : Float(768, 3072) = aten::t(%4474), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.29 : Float(1, 128, 3072) = aten::matmul(%input_tensor.5, %4475), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4477 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.51 : Float(1, 128, 3072) = aten::add_(%output.29, %4473, %4477), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.52 : Float(1, 128, 3072) = aten::gelu(%input.51), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %4480 : __torch__.torch.nn.modules.normalization.___torch_mangle_1806.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4336)\n",
      "  %4481 : __torch__.torch.nn.modules.dropout.___torch_mangle_1807.Dropout = prim::GetAttr[name=\"dropout\"](%4336)\n",
      "  %4482 : __torch__.torch.nn.modules.linear.___torch_mangle_1805.Linear = prim::GetAttr[name=\"dense\"](%4336)\n",
      "  %4483 : Tensor = prim::GetAttr[name=\"bias\"](%4482)\n",
      "  %4484 : Tensor = prim::GetAttr[name=\"weight\"](%4482)\n",
      "  %4485 : Float(3072, 768) = aten::t(%4484), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.30 : Float(1, 128, 768) = aten::matmul(%input.52, %4485), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4487 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.53 : Float(1, 128, 768) = aten::add_(%output.30, %4483, %4487), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4489 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4490 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.10 : Float(1, 128, 768) = aten::dropout(%input.53, %4489, %4490), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.54 : Float(1, 128, 768) = aten::add(%hidden_states.10, %input_tensor.5, %4492), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %4494 : Tensor = prim::GetAttr[name=\"bias\"](%4480)\n",
      "  %4495 : Tensor = prim::GetAttr[name=\"weight\"](%4480)\n",
      "  %4496 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4497 : int[] = prim::ListConstruct(%4496), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm\n",
      "  %4498 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4499 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.55 : Float(1, 128, 768) = aten::layer_norm(%input.54, %4497, %4495, %4494, %4498, %4499), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4501 : __torch__.transformers.modeling_bert.___torch_mangle_1825.BertOutput = prim::GetAttr[name=\"output\"](%3665)\n",
      "  %4502 : __torch__.transformers.modeling_bert.___torch_mangle_1821.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3665)\n",
      "  %4503 : __torch__.transformers.modeling_bert.___torch_mangle_1819.BertAttention = prim::GetAttr[name=\"attention\"](%3665)\n",
      "  %4504 : __torch__.transformers.modeling_bert.___torch_mangle_1818.BertSelfOutput = prim::GetAttr[name=\"output\"](%4503)\n",
      "  %4505 : __torch__.transformers.modeling_bert.___torch_mangle_1814.BertSelfAttention = prim::GetAttr[name=\"self\"](%4503)\n",
      "  %4506 : __torch__.torch.nn.modules.dropout.___torch_mangle_1813.Dropout = prim::GetAttr[name=\"dropout\"](%4505)\n",
      "  %4507 : __torch__.torch.nn.modules.linear.___torch_mangle_1812.Linear = prim::GetAttr[name=\"value\"](%4505)\n",
      "  %4508 : __torch__.torch.nn.modules.linear.___torch_mangle_1811.Linear = prim::GetAttr[name=\"key\"](%4505)\n",
      "  %4509 : __torch__.torch.nn.modules.linear.___torch_mangle_1810.Linear = prim::GetAttr[name=\"query\"](%4505)\n",
      "  %4510 : Tensor = prim::GetAttr[name=\"bias\"](%4509)\n",
      "  %4511 : Tensor = prim::GetAttr[name=\"weight\"](%4509)\n",
      "  %4512 : Float(768, 768) = aten::t(%4511), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.31 : Float(1, 128, 768) = aten::matmul(%input.55, %4512), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4514 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.31 : Float(1, 128, 768) = aten::add_(%output.31, %4510, %4514), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4516 : Tensor = prim::GetAttr[name=\"bias\"](%4508)\n",
      "  %4517 : Tensor = prim::GetAttr[name=\"weight\"](%4508)\n",
      "  %4518 : Float(768, 768) = aten::t(%4517), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.32 : Float(1, 128, 768) = aten::matmul(%input.55, %4518), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4520 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.33 : Float(1, 128, 768) = aten::add_(%output.32, %4516, %4520), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4522 : Tensor = prim::GetAttr[name=\"bias\"](%4507)\n",
      "  %4523 : Tensor = prim::GetAttr[name=\"weight\"](%4507)\n",
      "  %4524 : Float(768, 768) = aten::t(%4523), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.33 : Float(1, 128, 768) = aten::matmul(%input.55, %4524), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4526 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.35 : Float(1, 128, 768) = aten::add_(%output.33, %4522, %4526), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4528 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4529 : int = aten::size(%x.31, %4528), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4530 : Long() = prim::NumToTensor(%4529), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4531 : int = aten::Int(%4530), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4532 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4533 : int = aten::size(%x.31, %4532), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4534 : Long() = prim::NumToTensor(%4533), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4535 : int = aten::Int(%4534), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4536 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4537 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4538 : int[] = prim::ListConstruct(%4531, %4535, %4536, %4537), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %x.32 : Float(1, 128, 12, 64) = aten::view(%x.31, %4538), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4540 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4541 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4542 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4543 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4544 : int[] = prim::ListConstruct(%4540, %4541, %4542, %4543), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %query_layer.6 : Float(1, 12, 128, 64) = aten::permute(%x.32, %4544), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4546 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4547 : int = aten::size(%x.33, %4546), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4548 : Long() = prim::NumToTensor(%4547), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4549 : int = aten::Int(%4548), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4551 : int = aten::size(%x.33, %4550), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4552 : Long() = prim::NumToTensor(%4551), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4553 : int = aten::Int(%4552), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4554 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4555 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4556 : int[] = prim::ListConstruct(%4549, %4553, %4554, %4555), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %x.34 : Float(1, 128, 12, 64) = aten::view(%x.33, %4556), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4558 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4559 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4560 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4561 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4562 : int[] = prim::ListConstruct(%4558, %4559, %4560, %4561), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %key_layer.6 : Float(1, 12, 128, 64) = aten::permute(%x.34, %4562), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4564 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4565 : int = aten::size(%x.35, %4564), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4566 : Long() = prim::NumToTensor(%4565), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4567 : int = aten::Int(%4566), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4568 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4569 : int = aten::size(%x.35, %4568), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4570 : Long() = prim::NumToTensor(%4569), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4571 : int = aten::Int(%4570), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4572 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4573 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4574 : int[] = prim::ListConstruct(%4567, %4571, %4572, %4573), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %x.36 : Float(1, 128, 12, 64) = aten::view(%x.35, %4574), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4576 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4577 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4578 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4579 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4580 : int[] = prim::ListConstruct(%4576, %4577, %4578, %4579), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %value_layer.6 : Float(1, 12, 128, 64) = aten::permute(%x.36, %4580), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4582 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4583 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4584 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.6, %4582, %4583), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.11 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.6, %4584), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4586 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.12 : Float(1, 12, 128, 128) = aten::div(%attention_scores.11, %4586), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %4588 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.56 : Float(1, 12, 128, 128) = aten::add(%attention_scores.12, %attention_mask, %4588), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %4590 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4591 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %input.57 : Float(1, 12, 128, 128) = aten::softmax(%input.56, %4590, %4591), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4593 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4594 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.6 : Float(1, 12, 128, 128) = aten::dropout(%input.57, %4593, %4594), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.11 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %4597 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4598 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4599 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4600 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4601 : int[] = prim::ListConstruct(%4597, %4598, %4599, %4600), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4602 : Float(1, 128, 12, 64) = aten::permute(%context_layer.11, %4601), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4603 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.12 : Float(1, 128, 12, 64) = aten::contiguous(%4602, %4603), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4605 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4606 : int = aten::size(%context_layer.12, %4605), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4607 : Long() = prim::NumToTensor(%4606), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4608 : int = aten::Int(%4607), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4609 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4610 : int = aten::size(%context_layer.12, %4609), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4611 : Long() = prim::NumToTensor(%4610), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4612 : int = aten::Int(%4611), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %4613 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4614 : int[] = prim::ListConstruct(%4608, %4612, %4613), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self\n",
      "  %input.58 : Float(1, 128, 768) = aten::view(%context_layer.12, %4614), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4616 : __torch__.torch.nn.modules.normalization.___torch_mangle_1816.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4504)\n",
      "  %4617 : __torch__.torch.nn.modules.dropout.___torch_mangle_1817.Dropout = prim::GetAttr[name=\"dropout\"](%4504)\n",
      "  %4618 : __torch__.torch.nn.modules.linear.___torch_mangle_1815.Linear = prim::GetAttr[name=\"dense\"](%4504)\n",
      "  %4619 : Tensor = prim::GetAttr[name=\"bias\"](%4618)\n",
      "  %4620 : Tensor = prim::GetAttr[name=\"weight\"](%4618)\n",
      "  %4621 : Float(768, 768) = aten::t(%4620), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.34 : Float(1, 128, 768) = aten::matmul(%input.58, %4621), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4623 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.59 : Float(1, 128, 768) = aten::add_(%output.34, %4619, %4623), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4625 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4626 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.11 : Float(1, 128, 768) = aten::dropout(%input.59, %4625, %4626), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4628 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.60 : Float(1, 128, 768) = aten::add(%hidden_states.11, %input.55, %4628), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %4630 : Tensor = prim::GetAttr[name=\"bias\"](%4616)\n",
      "  %4631 : Tensor = prim::GetAttr[name=\"weight\"](%4616)\n",
      "  %4632 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4633 : int[] = prim::ListConstruct(%4632), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm\n",
      "  %4634 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4635 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.6 : Float(1, 128, 768) = aten::layer_norm(%input.60, %4633, %4631, %4630, %4634, %4635), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4637 : __torch__.torch.nn.modules.linear.___torch_mangle_1820.Linear = prim::GetAttr[name=\"dense\"](%4502)\n",
      "  %4638 : Tensor = prim::GetAttr[name=\"bias\"](%4637)\n",
      "  %4639 : Tensor = prim::GetAttr[name=\"weight\"](%4637)\n",
      "  %4640 : Float(768, 3072) = aten::t(%4639), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.35 : Float(1, 128, 3072) = aten::matmul(%input_tensor.6, %4640), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4642 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.61 : Float(1, 128, 3072) = aten::add_(%output.35, %4638, %4642), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.62 : Float(1, 128, 3072) = aten::gelu(%input.61), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %4645 : __torch__.torch.nn.modules.normalization.___torch_mangle_1823.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4501)\n",
      "  %4646 : __torch__.torch.nn.modules.dropout.___torch_mangle_1824.Dropout = prim::GetAttr[name=\"dropout\"](%4501)\n",
      "  %4647 : __torch__.torch.nn.modules.linear.___torch_mangle_1822.Linear = prim::GetAttr[name=\"dense\"](%4501)\n",
      "  %4648 : Tensor = prim::GetAttr[name=\"bias\"](%4647)\n",
      "  %4649 : Tensor = prim::GetAttr[name=\"weight\"](%4647)\n",
      "  %4650 : Float(3072, 768) = aten::t(%4649), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.36 : Float(1, 128, 768) = aten::matmul(%input.62, %4650), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4652 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.63 : Float(1, 128, 768) = aten::add_(%output.36, %4648, %4652), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4654 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4655 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.12 : Float(1, 128, 768) = aten::dropout(%input.63, %4654, %4655), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4657 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.64 : Float(1, 128, 768) = aten::add(%hidden_states.12, %input_tensor.6, %4657), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %4659 : Tensor = prim::GetAttr[name=\"bias\"](%4645)\n",
      "  %4660 : Tensor = prim::GetAttr[name=\"weight\"](%4645)\n",
      "  %4661 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4662 : int[] = prim::ListConstruct(%4661), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm\n",
      "  %4663 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4664 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.65 : Float(1, 128, 768) = aten::layer_norm(%input.64, %4662, %4660, %4659, %4663, %4664), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4666 : __torch__.transformers.modeling_bert.___torch_mangle_1842.BertOutput = prim::GetAttr[name=\"output\"](%3663)\n",
      "  %4667 : __torch__.transformers.modeling_bert.___torch_mangle_1838.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3663)\n",
      "  %4668 : __torch__.transformers.modeling_bert.___torch_mangle_1836.BertAttention = prim::GetAttr[name=\"attention\"](%3663)\n",
      "  %4669 : __torch__.transformers.modeling_bert.___torch_mangle_1835.BertSelfOutput = prim::GetAttr[name=\"output\"](%4668)\n",
      "  %4670 : __torch__.transformers.modeling_bert.___torch_mangle_1831.BertSelfAttention = prim::GetAttr[name=\"self\"](%4668)\n",
      "  %4671 : __torch__.torch.nn.modules.dropout.___torch_mangle_1830.Dropout = prim::GetAttr[name=\"dropout\"](%4670)\n",
      "  %4672 : __torch__.torch.nn.modules.linear.___torch_mangle_1829.Linear = prim::GetAttr[name=\"value\"](%4670)\n",
      "  %4673 : __torch__.torch.nn.modules.linear.___torch_mangle_1828.Linear = prim::GetAttr[name=\"key\"](%4670)\n",
      "  %4674 : __torch__.torch.nn.modules.linear.___torch_mangle_1827.Linear = prim::GetAttr[name=\"query\"](%4670)\n",
      "  %4675 : Tensor = prim::GetAttr[name=\"bias\"](%4674)\n",
      "  %4676 : Tensor = prim::GetAttr[name=\"weight\"](%4674)\n",
      "  %4677 : Float(768, 768) = aten::t(%4676), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.37 : Float(1, 128, 768) = aten::matmul(%input.65, %4677), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.37 : Float(1, 128, 768) = aten::add_(%output.37, %4675, %4679), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4681 : Tensor = prim::GetAttr[name=\"bias\"](%4673)\n",
      "  %4682 : Tensor = prim::GetAttr[name=\"weight\"](%4673)\n",
      "  %4683 : Float(768, 768) = aten::t(%4682), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.38 : Float(1, 128, 768) = aten::matmul(%input.65, %4683), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4685 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.39 : Float(1, 128, 768) = aten::add_(%output.38, %4681, %4685), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4687 : Tensor = prim::GetAttr[name=\"bias\"](%4672)\n",
      "  %4688 : Tensor = prim::GetAttr[name=\"weight\"](%4672)\n",
      "  %4689 : Float(768, 768) = aten::t(%4688), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.39 : Float(1, 128, 768) = aten::matmul(%input.65, %4689), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4691 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.41 : Float(1, 128, 768) = aten::add_(%output.39, %4687, %4691), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4693 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4694 : int = aten::size(%x.37, %4693), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4695 : Long() = prim::NumToTensor(%4694), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4696 : int = aten::Int(%4695), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4697 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4698 : int = aten::size(%x.37, %4697), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4699 : Long() = prim::NumToTensor(%4698), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4700 : int = aten::Int(%4699), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4701 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4702 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4703 : int[] = prim::ListConstruct(%4696, %4700, %4701, %4702), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %x.38 : Float(1, 128, 12, 64) = aten::view(%x.37, %4703), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4705 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4706 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4708 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4709 : int[] = prim::ListConstruct(%4705, %4706, %4707, %4708), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %query_layer.7 : Float(1, 12, 128, 64) = aten::permute(%x.38, %4709), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4711 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4712 : int = aten::size(%x.39, %4711), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4713 : Long() = prim::NumToTensor(%4712), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4714 : int = aten::Int(%4713), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4715 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4716 : int = aten::size(%x.39, %4715), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4717 : Long() = prim::NumToTensor(%4716), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4718 : int = aten::Int(%4717), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4719 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4720 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4721 : int[] = prim::ListConstruct(%4714, %4718, %4719, %4720), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %x.40 : Float(1, 128, 12, 64) = aten::view(%x.39, %4721), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4723 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4724 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4725 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4726 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4727 : int[] = prim::ListConstruct(%4723, %4724, %4725, %4726), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %key_layer.7 : Float(1, 12, 128, 64) = aten::permute(%x.40, %4727), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4729 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4730 : int = aten::size(%x.41, %4729), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4731 : Long() = prim::NumToTensor(%4730), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4732 : int = aten::Int(%4731), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4733 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4734 : int = aten::size(%x.41, %4733), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4735 : Long() = prim::NumToTensor(%4734), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4736 : int = aten::Int(%4735), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4737 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4738 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4739 : int[] = prim::ListConstruct(%4732, %4736, %4737, %4738), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %x.42 : Float(1, 128, 12, 64) = aten::view(%x.41, %4739), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4741 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4743 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4744 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4745 : int[] = prim::ListConstruct(%4741, %4742, %4743, %4744), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %value_layer.7 : Float(1, 12, 128, 64) = aten::permute(%x.42, %4745), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4747 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4748 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4749 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.7, %4747, %4748), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.13 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.7, %4749), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4751 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.14 : Float(1, 12, 128, 128) = aten::div(%attention_scores.13, %4751), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %4753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.66 : Float(1, 12, 128, 128) = aten::add(%attention_scores.14, %attention_mask, %4753), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %4755 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4756 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %input.67 : Float(1, 12, 128, 128) = aten::softmax(%input.66, %4755, %4756), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4758 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4759 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.7 : Float(1, 12, 128, 128) = aten::dropout(%input.67, %4758, %4759), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.13 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %4762 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4763 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4764 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4765 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4766 : int[] = prim::ListConstruct(%4762, %4763, %4764, %4765), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4767 : Float(1, 128, 12, 64) = aten::permute(%context_layer.13, %4766), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4768 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.14 : Float(1, 128, 12, 64) = aten::contiguous(%4767, %4768), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4770 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4771 : int = aten::size(%context_layer.14, %4770), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4772 : Long() = prim::NumToTensor(%4771), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4773 : int = aten::Int(%4772), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4775 : int = aten::size(%context_layer.14, %4774), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4776 : Long() = prim::NumToTensor(%4775), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4777 : int = aten::Int(%4776), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %4778 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4779 : int[] = prim::ListConstruct(%4773, %4777, %4778), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self\n",
      "  %input.68 : Float(1, 128, 768) = aten::view(%context_layer.14, %4779), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4781 : __torch__.torch.nn.modules.normalization.___torch_mangle_1833.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4669)\n",
      "  %4782 : __torch__.torch.nn.modules.dropout.___torch_mangle_1834.Dropout = prim::GetAttr[name=\"dropout\"](%4669)\n",
      "  %4783 : __torch__.torch.nn.modules.linear.___torch_mangle_1832.Linear = prim::GetAttr[name=\"dense\"](%4669)\n",
      "  %4784 : Tensor = prim::GetAttr[name=\"bias\"](%4783)\n",
      "  %4785 : Tensor = prim::GetAttr[name=\"weight\"](%4783)\n",
      "  %4786 : Float(768, 768) = aten::t(%4785), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.40 : Float(1, 128, 768) = aten::matmul(%input.68, %4786), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4788 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.69 : Float(1, 128, 768) = aten::add_(%output.40, %4784, %4788), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4790 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4791 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.13 : Float(1, 128, 768) = aten::dropout(%input.69, %4790, %4791), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4793 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.70 : Float(1, 128, 768) = aten::add(%hidden_states.13, %input.65, %4793), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %4795 : Tensor = prim::GetAttr[name=\"bias\"](%4781)\n",
      "  %4796 : Tensor = prim::GetAttr[name=\"weight\"](%4781)\n",
      "  %4797 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4798 : int[] = prim::ListConstruct(%4797), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm\n",
      "  %4799 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4800 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.7 : Float(1, 128, 768) = aten::layer_norm(%input.70, %4798, %4796, %4795, %4799, %4800), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4802 : __torch__.torch.nn.modules.linear.___torch_mangle_1837.Linear = prim::GetAttr[name=\"dense\"](%4667)\n",
      "  %4803 : Tensor = prim::GetAttr[name=\"bias\"](%4802)\n",
      "  %4804 : Tensor = prim::GetAttr[name=\"weight\"](%4802)\n",
      "  %4805 : Float(768, 3072) = aten::t(%4804), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.41 : Float(1, 128, 3072) = aten::matmul(%input_tensor.7, %4805), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4807 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.71 : Float(1, 128, 3072) = aten::add_(%output.41, %4803, %4807), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.72 : Float(1, 128, 3072) = aten::gelu(%input.71), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %4810 : __torch__.torch.nn.modules.normalization.___torch_mangle_1840.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4666)\n",
      "  %4811 : __torch__.torch.nn.modules.dropout.___torch_mangle_1841.Dropout = prim::GetAttr[name=\"dropout\"](%4666)\n",
      "  %4812 : __torch__.torch.nn.modules.linear.___torch_mangle_1839.Linear = prim::GetAttr[name=\"dense\"](%4666)\n",
      "  %4813 : Tensor = prim::GetAttr[name=\"bias\"](%4812)\n",
      "  %4814 : Tensor = prim::GetAttr[name=\"weight\"](%4812)\n",
      "  %4815 : Float(3072, 768) = aten::t(%4814), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.42 : Float(1, 128, 768) = aten::matmul(%input.72, %4815), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4817 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.73 : Float(1, 128, 768) = aten::add_(%output.42, %4813, %4817), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4819 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4820 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.14 : Float(1, 128, 768) = aten::dropout(%input.73, %4819, %4820), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4822 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.74 : Float(1, 128, 768) = aten::add(%hidden_states.14, %input_tensor.7, %4822), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %4824 : Tensor = prim::GetAttr[name=\"bias\"](%4810)\n",
      "  %4825 : Tensor = prim::GetAttr[name=\"weight\"](%4810)\n",
      "  %4826 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4827 : int[] = prim::ListConstruct(%4826), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm\n",
      "  %4828 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4829 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.75 : Float(1, 128, 768) = aten::layer_norm(%input.74, %4827, %4825, %4824, %4828, %4829), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4831 : __torch__.transformers.modeling_bert.___torch_mangle_1859.BertOutput = prim::GetAttr[name=\"output\"](%3661)\n",
      "  %4832 : __torch__.transformers.modeling_bert.___torch_mangle_1855.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3661)\n",
      "  %4833 : __torch__.transformers.modeling_bert.___torch_mangle_1853.BertAttention = prim::GetAttr[name=\"attention\"](%3661)\n",
      "  %4834 : __torch__.transformers.modeling_bert.___torch_mangle_1852.BertSelfOutput = prim::GetAttr[name=\"output\"](%4833)\n",
      "  %4835 : __torch__.transformers.modeling_bert.___torch_mangle_1848.BertSelfAttention = prim::GetAttr[name=\"self\"](%4833)\n",
      "  %4836 : __torch__.torch.nn.modules.dropout.___torch_mangle_1847.Dropout = prim::GetAttr[name=\"dropout\"](%4835)\n",
      "  %4837 : __torch__.torch.nn.modules.linear.___torch_mangle_1846.Linear = prim::GetAttr[name=\"value\"](%4835)\n",
      "  %4838 : __torch__.torch.nn.modules.linear.___torch_mangle_1845.Linear = prim::GetAttr[name=\"key\"](%4835)\n",
      "  %4839 : __torch__.torch.nn.modules.linear.___torch_mangle_1844.Linear = prim::GetAttr[name=\"query\"](%4835)\n",
      "  %4840 : Tensor = prim::GetAttr[name=\"bias\"](%4839)\n",
      "  %4841 : Tensor = prim::GetAttr[name=\"weight\"](%4839)\n",
      "  %4842 : Float(768, 768) = aten::t(%4841), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.43 : Float(1, 128, 768) = aten::matmul(%input.75, %4842), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.43 : Float(1, 128, 768) = aten::add_(%output.43, %4840, %4844), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4846 : Tensor = prim::GetAttr[name=\"bias\"](%4838)\n",
      "  %4847 : Tensor = prim::GetAttr[name=\"weight\"](%4838)\n",
      "  %4848 : Float(768, 768) = aten::t(%4847), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.44 : Float(1, 128, 768) = aten::matmul(%input.75, %4848), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4850 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.45 : Float(1, 128, 768) = aten::add_(%output.44, %4846, %4850), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4852 : Tensor = prim::GetAttr[name=\"bias\"](%4837)\n",
      "  %4853 : Tensor = prim::GetAttr[name=\"weight\"](%4837)\n",
      "  %4854 : Float(768, 768) = aten::t(%4853), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.45 : Float(1, 128, 768) = aten::matmul(%input.75, %4854), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4856 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.47 : Float(1, 128, 768) = aten::add_(%output.45, %4852, %4856), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4858 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4859 : int = aten::size(%x.43, %4858), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4860 : Long() = prim::NumToTensor(%4859), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4861 : int = aten::Int(%4860), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4862 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4863 : int = aten::size(%x.43, %4862), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4864 : Long() = prim::NumToTensor(%4863), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4865 : int = aten::Int(%4864), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4866 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4867 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4868 : int[] = prim::ListConstruct(%4861, %4865, %4866, %4867), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %x.44 : Float(1, 128, 12, 64) = aten::view(%x.43, %4868), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4870 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4871 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4872 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4873 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4874 : int[] = prim::ListConstruct(%4870, %4871, %4872, %4873), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %query_layer.8 : Float(1, 12, 128, 64) = aten::permute(%x.44, %4874), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4876 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4877 : int = aten::size(%x.45, %4876), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4878 : Long() = prim::NumToTensor(%4877), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4879 : int = aten::Int(%4878), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4880 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4881 : int = aten::size(%x.45, %4880), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4882 : Long() = prim::NumToTensor(%4881), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4883 : int = aten::Int(%4882), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4884 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4885 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4886 : int[] = prim::ListConstruct(%4879, %4883, %4884, %4885), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %x.46 : Float(1, 128, 12, 64) = aten::view(%x.45, %4886), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4889 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4891 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4892 : int[] = prim::ListConstruct(%4888, %4889, %4890, %4891), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %key_layer.8 : Float(1, 12, 128, 64) = aten::permute(%x.46, %4892), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4894 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4895 : int = aten::size(%x.47, %4894), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4896 : Long() = prim::NumToTensor(%4895), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4897 : int = aten::Int(%4896), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4898 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4899 : int = aten::size(%x.47, %4898), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %4900 : Long() = prim::NumToTensor(%4899), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4901 : int = aten::Int(%4900), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4902 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4903 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4904 : int[] = prim::ListConstruct(%4897, %4901, %4902, %4903), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %x.48 : Float(1, 128, 12, 64) = aten::view(%x.47, %4904), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %4906 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4907 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4909 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4910 : int[] = prim::ListConstruct(%4906, %4907, %4908, %4909), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %value_layer.8 : Float(1, 12, 128, 64) = aten::permute(%x.48, %4910), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %4912 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4913 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4914 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.8, %4912, %4913), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.15 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.8, %4914), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %4916 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.16 : Float(1, 12, 128, 128) = aten::div(%attention_scores.15, %4916), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %4918 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.76 : Float(1, 12, 128, 128) = aten::add(%attention_scores.16, %attention_mask, %4918), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %4920 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4921 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %input.77 : Float(1, 12, 128, 128) = aten::softmax(%input.76, %4920, %4921), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %4923 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4924 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.8 : Float(1, 12, 128, 128) = aten::dropout(%input.77, %4923, %4924), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.15 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %4927 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4928 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4929 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4930 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4931 : int[] = prim::ListConstruct(%4927, %4928, %4929, %4930), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4932 : Float(1, 128, 12, 64) = aten::permute(%context_layer.15, %4931), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4933 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.16 : Float(1, 128, 12, 64) = aten::contiguous(%4932, %4933), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %4935 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4936 : int = aten::size(%context_layer.16, %4935), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4937 : Long() = prim::NumToTensor(%4936), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4938 : int = aten::Int(%4937), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4939 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4940 : int = aten::size(%context_layer.16, %4939), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %4941 : Long() = prim::NumToTensor(%4940), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4942 : int = aten::Int(%4941), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %4943 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4944 : int[] = prim::ListConstruct(%4938, %4942, %4943), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self\n",
      "  %input.78 : Float(1, 128, 768) = aten::view(%context_layer.16, %4944), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %4946 : __torch__.torch.nn.modules.normalization.___torch_mangle_1850.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4834)\n",
      "  %4947 : __torch__.torch.nn.modules.dropout.___torch_mangle_1851.Dropout = prim::GetAttr[name=\"dropout\"](%4834)\n",
      "  %4948 : __torch__.torch.nn.modules.linear.___torch_mangle_1849.Linear = prim::GetAttr[name=\"dense\"](%4834)\n",
      "  %4949 : Tensor = prim::GetAttr[name=\"bias\"](%4948)\n",
      "  %4950 : Tensor = prim::GetAttr[name=\"weight\"](%4948)\n",
      "  %4951 : Float(768, 768) = aten::t(%4950), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.46 : Float(1, 128, 768) = aten::matmul(%input.78, %4951), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4953 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.79 : Float(1, 128, 768) = aten::add_(%output.46, %4949, %4953), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4955 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4956 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.15 : Float(1, 128, 768) = aten::dropout(%input.79, %4955, %4956), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.80 : Float(1, 128, 768) = aten::add(%hidden_states.15, %input.75, %4958), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %4960 : Tensor = prim::GetAttr[name=\"bias\"](%4946)\n",
      "  %4961 : Tensor = prim::GetAttr[name=\"weight\"](%4946)\n",
      "  %4962 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4963 : int[] = prim::ListConstruct(%4962), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm\n",
      "  %4964 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4965 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.8 : Float(1, 128, 768) = aten::layer_norm(%input.80, %4963, %4961, %4960, %4964, %4965), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4967 : __torch__.torch.nn.modules.linear.___torch_mangle_1854.Linear = prim::GetAttr[name=\"dense\"](%4832)\n",
      "  %4968 : Tensor = prim::GetAttr[name=\"bias\"](%4967)\n",
      "  %4969 : Tensor = prim::GetAttr[name=\"weight\"](%4967)\n",
      "  %4970 : Float(768, 3072) = aten::t(%4969), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.47 : Float(1, 128, 3072) = aten::matmul(%input_tensor.8, %4970), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4972 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.81 : Float(1, 128, 3072) = aten::add_(%output.47, %4968, %4972), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.82 : Float(1, 128, 3072) = aten::gelu(%input.81), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %4975 : __torch__.torch.nn.modules.normalization.___torch_mangle_1857.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4831)\n",
      "  %4976 : __torch__.torch.nn.modules.dropout.___torch_mangle_1858.Dropout = prim::GetAttr[name=\"dropout\"](%4831)\n",
      "  %4977 : __torch__.torch.nn.modules.linear.___torch_mangle_1856.Linear = prim::GetAttr[name=\"dense\"](%4831)\n",
      "  %4978 : Tensor = prim::GetAttr[name=\"bias\"](%4977)\n",
      "  %4979 : Tensor = prim::GetAttr[name=\"weight\"](%4977)\n",
      "  %4980 : Float(3072, 768) = aten::t(%4979), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.48 : Float(1, 128, 768) = aten::matmul(%input.82, %4980), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %4982 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.83 : Float(1, 128, 768) = aten::add_(%output.48, %4978, %4982), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %4984 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4985 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.16 : Float(1, 128, 768) = aten::dropout(%input.83, %4984, %4985), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %4987 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.84 : Float(1, 128, 768) = aten::add(%hidden_states.16, %input_tensor.8, %4987), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %4989 : Tensor = prim::GetAttr[name=\"bias\"](%4975)\n",
      "  %4990 : Tensor = prim::GetAttr[name=\"weight\"](%4975)\n",
      "  %4991 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4992 : int[] = prim::ListConstruct(%4991), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm\n",
      "  %4993 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4994 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.85 : Float(1, 128, 768) = aten::layer_norm(%input.84, %4992, %4990, %4989, %4993, %4994), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %4996 : __torch__.transformers.modeling_bert.___torch_mangle_1876.BertOutput = prim::GetAttr[name=\"output\"](%3659)\n",
      "  %4997 : __torch__.transformers.modeling_bert.___torch_mangle_1872.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3659)\n",
      "  %4998 : __torch__.transformers.modeling_bert.___torch_mangle_1870.BertAttention = prim::GetAttr[name=\"attention\"](%3659)\n",
      "  %4999 : __torch__.transformers.modeling_bert.___torch_mangle_1869.BertSelfOutput = prim::GetAttr[name=\"output\"](%4998)\n",
      "  %5000 : __torch__.transformers.modeling_bert.___torch_mangle_1865.BertSelfAttention = prim::GetAttr[name=\"self\"](%4998)\n",
      "  %5001 : __torch__.torch.nn.modules.dropout.___torch_mangle_1864.Dropout = prim::GetAttr[name=\"dropout\"](%5000)\n",
      "  %5002 : __torch__.torch.nn.modules.linear.___torch_mangle_1863.Linear = prim::GetAttr[name=\"value\"](%5000)\n",
      "  %5003 : __torch__.torch.nn.modules.linear.___torch_mangle_1862.Linear = prim::GetAttr[name=\"key\"](%5000)\n",
      "  %5004 : __torch__.torch.nn.modules.linear.___torch_mangle_1861.Linear = prim::GetAttr[name=\"query\"](%5000)\n",
      "  %5005 : Tensor = prim::GetAttr[name=\"bias\"](%5004)\n",
      "  %5006 : Tensor = prim::GetAttr[name=\"weight\"](%5004)\n",
      "  %5007 : Float(768, 768) = aten::t(%5006), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.49 : Float(1, 128, 768) = aten::matmul(%input.85, %5007), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5009 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.49 : Float(1, 128, 768) = aten::add_(%output.49, %5005, %5009), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5011 : Tensor = prim::GetAttr[name=\"bias\"](%5003)\n",
      "  %5012 : Tensor = prim::GetAttr[name=\"weight\"](%5003)\n",
      "  %5013 : Float(768, 768) = aten::t(%5012), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.50 : Float(1, 128, 768) = aten::matmul(%input.85, %5013), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5015 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.51 : Float(1, 128, 768) = aten::add_(%output.50, %5011, %5015), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5017 : Tensor = prim::GetAttr[name=\"bias\"](%5002)\n",
      "  %5018 : Tensor = prim::GetAttr[name=\"weight\"](%5002)\n",
      "  %5019 : Float(768, 768) = aten::t(%5018), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.51 : Float(1, 128, 768) = aten::matmul(%input.85, %5019), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5021 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.53 : Float(1, 128, 768) = aten::add_(%output.51, %5017, %5021), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5023 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5024 : int = aten::size(%x.49, %5023), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5025 : Long() = prim::NumToTensor(%5024), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5026 : int = aten::Int(%5025), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5027 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5028 : int = aten::size(%x.49, %5027), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5029 : Long() = prim::NumToTensor(%5028), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5030 : int = aten::Int(%5029), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5031 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5032 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5033 : int[] = prim::ListConstruct(%5026, %5030, %5031, %5032), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %x.50 : Float(1, 128, 12, 64) = aten::view(%x.49, %5033), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5035 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5036 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5037 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5038 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5039 : int[] = prim::ListConstruct(%5035, %5036, %5037, %5038), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %query_layer.9 : Float(1, 12, 128, 64) = aten::permute(%x.50, %5039), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5041 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5042 : int = aten::size(%x.51, %5041), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5043 : Long() = prim::NumToTensor(%5042), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5044 : int = aten::Int(%5043), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5045 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5046 : int = aten::size(%x.51, %5045), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5047 : Long() = prim::NumToTensor(%5046), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5048 : int = aten::Int(%5047), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5049 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5050 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5051 : int[] = prim::ListConstruct(%5044, %5048, %5049, %5050), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %x.52 : Float(1, 128, 12, 64) = aten::view(%x.51, %5051), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5053 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5054 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5055 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5056 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5057 : int[] = prim::ListConstruct(%5053, %5054, %5055, %5056), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %key_layer.9 : Float(1, 12, 128, 64) = aten::permute(%x.52, %5057), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5059 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5060 : int = aten::size(%x.53, %5059), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5061 : Long() = prim::NumToTensor(%5060), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5062 : int = aten::Int(%5061), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5064 : int = aten::size(%x.53, %5063), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5065 : Long() = prim::NumToTensor(%5064), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5066 : int = aten::Int(%5065), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5067 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5068 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5069 : int[] = prim::ListConstruct(%5062, %5066, %5067, %5068), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %x.54 : Float(1, 128, 12, 64) = aten::view(%x.53, %5069), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5071 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5072 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5073 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5074 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5075 : int[] = prim::ListConstruct(%5071, %5072, %5073, %5074), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %value_layer.9 : Float(1, 12, 128, 64) = aten::permute(%x.54, %5075), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5077 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5078 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5079 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.9, %5077, %5078), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.17 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.9, %5079), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5081 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.18 : Float(1, 12, 128, 128) = aten::div(%attention_scores.17, %5081), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %5083 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.86 : Float(1, 12, 128, 128) = aten::add(%attention_scores.18, %attention_mask, %5083), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %5085 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %5086 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %input.87 : Float(1, 12, 128, 128) = aten::softmax(%input.86, %5085, %5086), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %5088 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5089 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.9 : Float(1, 12, 128, 128) = aten::dropout(%input.87, %5088, %5089), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.17 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %5092 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5093 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5094 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5095 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5096 : int[] = prim::ListConstruct(%5092, %5093, %5094, %5095), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5097 : Float(1, 128, 12, 64) = aten::permute(%context_layer.17, %5096), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5098 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.18 : Float(1, 128, 12, 64) = aten::contiguous(%5097, %5098), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5100 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5101 : int = aten::size(%context_layer.18, %5100), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5102 : Long() = prim::NumToTensor(%5101), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5103 : int = aten::Int(%5102), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5104 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5105 : int = aten::size(%context_layer.18, %5104), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5106 : Long() = prim::NumToTensor(%5105), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5107 : int = aten::Int(%5106), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %5108 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %5109 : int[] = prim::ListConstruct(%5103, %5107, %5108), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self\n",
      "  %input.88 : Float(1, 128, 768) = aten::view(%context_layer.18, %5109), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %5111 : __torch__.torch.nn.modules.normalization.___torch_mangle_1867.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4999)\n",
      "  %5112 : __torch__.torch.nn.modules.dropout.___torch_mangle_1868.Dropout = prim::GetAttr[name=\"dropout\"](%4999)\n",
      "  %5113 : __torch__.torch.nn.modules.linear.___torch_mangle_1866.Linear = prim::GetAttr[name=\"dense\"](%4999)\n",
      "  %5114 : Tensor = prim::GetAttr[name=\"bias\"](%5113)\n",
      "  %5115 : Tensor = prim::GetAttr[name=\"weight\"](%5113)\n",
      "  %5116 : Float(768, 768) = aten::t(%5115), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.52 : Float(1, 128, 768) = aten::matmul(%input.88, %5116), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5118 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.89 : Float(1, 128, 768) = aten::add_(%output.52, %5114, %5118), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5120 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5121 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.17 : Float(1, 128, 768) = aten::dropout(%input.89, %5120, %5121), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.90 : Float(1, 128, 768) = aten::add(%hidden_states.17, %input.85, %5123), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %5125 : Tensor = prim::GetAttr[name=\"bias\"](%5111)\n",
      "  %5126 : Tensor = prim::GetAttr[name=\"weight\"](%5111)\n",
      "  %5127 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5128 : int[] = prim::ListConstruct(%5127), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm\n",
      "  %5129 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5130 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.9 : Float(1, 128, 768) = aten::layer_norm(%input.90, %5128, %5126, %5125, %5129, %5130), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5132 : __torch__.torch.nn.modules.linear.___torch_mangle_1871.Linear = prim::GetAttr[name=\"dense\"](%4997)\n",
      "  %5133 : Tensor = prim::GetAttr[name=\"bias\"](%5132)\n",
      "  %5134 : Tensor = prim::GetAttr[name=\"weight\"](%5132)\n",
      "  %5135 : Float(768, 3072) = aten::t(%5134), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.53 : Float(1, 128, 3072) = aten::matmul(%input_tensor.9, %5135), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5137 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.91 : Float(1, 128, 3072) = aten::add_(%output.53, %5133, %5137), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.92 : Float(1, 128, 3072) = aten::gelu(%input.91), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %5140 : __torch__.torch.nn.modules.normalization.___torch_mangle_1874.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4996)\n",
      "  %5141 : __torch__.torch.nn.modules.dropout.___torch_mangle_1875.Dropout = prim::GetAttr[name=\"dropout\"](%4996)\n",
      "  %5142 : __torch__.torch.nn.modules.linear.___torch_mangle_1873.Linear = prim::GetAttr[name=\"dense\"](%4996)\n",
      "  %5143 : Tensor = prim::GetAttr[name=\"bias\"](%5142)\n",
      "  %5144 : Tensor = prim::GetAttr[name=\"weight\"](%5142)\n",
      "  %5145 : Float(3072, 768) = aten::t(%5144), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.54 : Float(1, 128, 768) = aten::matmul(%input.92, %5145), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5147 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.93 : Float(1, 128, 768) = aten::add_(%output.54, %5143, %5147), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5149 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5150 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.18 : Float(1, 128, 768) = aten::dropout(%input.93, %5149, %5150), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5152 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.94 : Float(1, 128, 768) = aten::add(%hidden_states.18, %input_tensor.9, %5152), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %5154 : Tensor = prim::GetAttr[name=\"bias\"](%5140)\n",
      "  %5155 : Tensor = prim::GetAttr[name=\"weight\"](%5140)\n",
      "  %5156 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5157 : int[] = prim::ListConstruct(%5156), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm\n",
      "  %5158 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5159 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.95 : Float(1, 128, 768) = aten::layer_norm(%input.94, %5157, %5155, %5154, %5158, %5159), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5161 : __torch__.transformers.modeling_bert.___torch_mangle_1893.BertOutput = prim::GetAttr[name=\"output\"](%3657)\n",
      "  %5162 : __torch__.transformers.modeling_bert.___torch_mangle_1889.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3657)\n",
      "  %5163 : __torch__.transformers.modeling_bert.___torch_mangle_1887.BertAttention = prim::GetAttr[name=\"attention\"](%3657)\n",
      "  %5164 : __torch__.transformers.modeling_bert.___torch_mangle_1886.BertSelfOutput = prim::GetAttr[name=\"output\"](%5163)\n",
      "  %5165 : __torch__.transformers.modeling_bert.___torch_mangle_1882.BertSelfAttention = prim::GetAttr[name=\"self\"](%5163)\n",
      "  %5166 : __torch__.torch.nn.modules.dropout.___torch_mangle_1881.Dropout = prim::GetAttr[name=\"dropout\"](%5165)\n",
      "  %5167 : __torch__.torch.nn.modules.linear.___torch_mangle_1880.Linear = prim::GetAttr[name=\"value\"](%5165)\n",
      "  %5168 : __torch__.torch.nn.modules.linear.___torch_mangle_1879.Linear = prim::GetAttr[name=\"key\"](%5165)\n",
      "  %5169 : __torch__.torch.nn.modules.linear.___torch_mangle_1878.Linear = prim::GetAttr[name=\"query\"](%5165)\n",
      "  %5170 : Tensor = prim::GetAttr[name=\"bias\"](%5169)\n",
      "  %5171 : Tensor = prim::GetAttr[name=\"weight\"](%5169)\n",
      "  %5172 : Float(768, 768) = aten::t(%5171), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.55 : Float(1, 128, 768) = aten::matmul(%input.95, %5172), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5174 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.55 : Float(1, 128, 768) = aten::add_(%output.55, %5170, %5174), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5176 : Tensor = prim::GetAttr[name=\"bias\"](%5168)\n",
      "  %5177 : Tensor = prim::GetAttr[name=\"weight\"](%5168)\n",
      "  %5178 : Float(768, 768) = aten::t(%5177), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.56 : Float(1, 128, 768) = aten::matmul(%input.95, %5178), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.57 : Float(1, 128, 768) = aten::add_(%output.56, %5176, %5180), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5182 : Tensor = prim::GetAttr[name=\"bias\"](%5167)\n",
      "  %5183 : Tensor = prim::GetAttr[name=\"weight\"](%5167)\n",
      "  %5184 : Float(768, 768) = aten::t(%5183), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.57 : Float(1, 128, 768) = aten::matmul(%input.95, %5184), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5186 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.59 : Float(1, 128, 768) = aten::add_(%output.57, %5182, %5186), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5188 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5189 : int = aten::size(%x.55, %5188), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5190 : Long() = prim::NumToTensor(%5189), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5191 : int = aten::Int(%5190), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5192 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5193 : int = aten::size(%x.55, %5192), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5194 : Long() = prim::NumToTensor(%5193), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5195 : int = aten::Int(%5194), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5196 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5197 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5198 : int[] = prim::ListConstruct(%5191, %5195, %5196, %5197), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %x.56 : Float(1, 128, 12, 64) = aten::view(%x.55, %5198), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5200 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5201 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5202 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5203 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5204 : int[] = prim::ListConstruct(%5200, %5201, %5202, %5203), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %query_layer.10 : Float(1, 12, 128, 64) = aten::permute(%x.56, %5204), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5206 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5207 : int = aten::size(%x.57, %5206), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5208 : Long() = prim::NumToTensor(%5207), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5209 : int = aten::Int(%5208), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5210 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5211 : int = aten::size(%x.57, %5210), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5212 : Long() = prim::NumToTensor(%5211), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5213 : int = aten::Int(%5212), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5214 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5216 : int[] = prim::ListConstruct(%5209, %5213, %5214, %5215), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %x.58 : Float(1, 128, 12, 64) = aten::view(%x.57, %5216), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5218 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5219 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5220 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5221 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5222 : int[] = prim::ListConstruct(%5218, %5219, %5220, %5221), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %key_layer.10 : Float(1, 12, 128, 64) = aten::permute(%x.58, %5222), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5224 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5225 : int = aten::size(%x.59, %5224), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5226 : Long() = prim::NumToTensor(%5225), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5227 : int = aten::Int(%5226), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5229 : int = aten::size(%x.59, %5228), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5230 : Long() = prim::NumToTensor(%5229), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5231 : int = aten::Int(%5230), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5232 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5233 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5234 : int[] = prim::ListConstruct(%5227, %5231, %5232, %5233), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %x.60 : Float(1, 128, 12, 64) = aten::view(%x.59, %5234), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5236 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5237 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5238 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5239 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5240 : int[] = prim::ListConstruct(%5236, %5237, %5238, %5239), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %value_layer.10 : Float(1, 12, 128, 64) = aten::permute(%x.60, %5240), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5242 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5243 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5244 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.10, %5242, %5243), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.19 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.10, %5244), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5246 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.20 : Float(1, 12, 128, 128) = aten::div(%attention_scores.19, %5246), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %5248 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.96 : Float(1, 12, 128, 128) = aten::add(%attention_scores.20, %attention_mask, %5248), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %5250 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %5251 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %input.97 : Float(1, 12, 128, 128) = aten::softmax(%input.96, %5250, %5251), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %5253 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5254 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.10 : Float(1, 12, 128, 128) = aten::dropout(%input.97, %5253, %5254), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.19 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %5257 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5258 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5260 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5261 : int[] = prim::ListConstruct(%5257, %5258, %5259, %5260), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5262 : Float(1, 128, 12, 64) = aten::permute(%context_layer.19, %5261), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5263 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.20 : Float(1, 128, 12, 64) = aten::contiguous(%5262, %5263), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5265 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5266 : int = aten::size(%context_layer.20, %5265), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5267 : Long() = prim::NumToTensor(%5266), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5268 : int = aten::Int(%5267), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5269 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5270 : int = aten::size(%context_layer.20, %5269), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5271 : Long() = prim::NumToTensor(%5270), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5272 : int = aten::Int(%5271), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %5273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %5274 : int[] = prim::ListConstruct(%5268, %5272, %5273), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self\n",
      "  %input.98 : Float(1, 128, 768) = aten::view(%context_layer.20, %5274), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %5276 : __torch__.torch.nn.modules.normalization.___torch_mangle_1884.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%5164)\n",
      "  %5277 : __torch__.torch.nn.modules.dropout.___torch_mangle_1885.Dropout = prim::GetAttr[name=\"dropout\"](%5164)\n",
      "  %5278 : __torch__.torch.nn.modules.linear.___torch_mangle_1883.Linear = prim::GetAttr[name=\"dense\"](%5164)\n",
      "  %5279 : Tensor = prim::GetAttr[name=\"bias\"](%5278)\n",
      "  %5280 : Tensor = prim::GetAttr[name=\"weight\"](%5278)\n",
      "  %5281 : Float(768, 768) = aten::t(%5280), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.58 : Float(1, 128, 768) = aten::matmul(%input.98, %5281), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.99 : Float(1, 128, 768) = aten::add_(%output.58, %5279, %5283), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5285 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5286 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.19 : Float(1, 128, 768) = aten::dropout(%input.99, %5285, %5286), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.100 : Float(1, 128, 768) = aten::add(%hidden_states.19, %input.95, %5288), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %5290 : Tensor = prim::GetAttr[name=\"bias\"](%5276)\n",
      "  %5291 : Tensor = prim::GetAttr[name=\"weight\"](%5276)\n",
      "  %5292 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5293 : int[] = prim::ListConstruct(%5292), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm\n",
      "  %5294 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5295 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.10 : Float(1, 128, 768) = aten::layer_norm(%input.100, %5293, %5291, %5290, %5294, %5295), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5297 : __torch__.torch.nn.modules.linear.___torch_mangle_1888.Linear = prim::GetAttr[name=\"dense\"](%5162)\n",
      "  %5298 : Tensor = prim::GetAttr[name=\"bias\"](%5297)\n",
      "  %5299 : Tensor = prim::GetAttr[name=\"weight\"](%5297)\n",
      "  %5300 : Float(768, 3072) = aten::t(%5299), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.59 : Float(1, 128, 3072) = aten::matmul(%input_tensor.10, %5300), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5302 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.101 : Float(1, 128, 3072) = aten::add_(%output.59, %5298, %5302), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.102 : Float(1, 128, 3072) = aten::gelu(%input.101), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %5305 : __torch__.torch.nn.modules.normalization.___torch_mangle_1891.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%5161)\n",
      "  %5306 : __torch__.torch.nn.modules.dropout.___torch_mangle_1892.Dropout = prim::GetAttr[name=\"dropout\"](%5161)\n",
      "  %5307 : __torch__.torch.nn.modules.linear.___torch_mangle_1890.Linear = prim::GetAttr[name=\"dense\"](%5161)\n",
      "  %5308 : Tensor = prim::GetAttr[name=\"bias\"](%5307)\n",
      "  %5309 : Tensor = prim::GetAttr[name=\"weight\"](%5307)\n",
      "  %5310 : Float(3072, 768) = aten::t(%5309), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.60 : Float(1, 128, 768) = aten::matmul(%input.102, %5310), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5312 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.103 : Float(1, 128, 768) = aten::add_(%output.60, %5308, %5312), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5314 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5315 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.20 : Float(1, 128, 768) = aten::dropout(%input.103, %5314, %5315), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.104 : Float(1, 128, 768) = aten::add(%hidden_states.20, %input_tensor.10, %5317), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %5319 : Tensor = prim::GetAttr[name=\"bias\"](%5305)\n",
      "  %5320 : Tensor = prim::GetAttr[name=\"weight\"](%5305)\n",
      "  %5321 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5322 : int[] = prim::ListConstruct(%5321), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm\n",
      "  %5323 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5324 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.105 : Float(1, 128, 768) = aten::layer_norm(%input.104, %5322, %5320, %5319, %5323, %5324), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5326 : __torch__.transformers.modeling_bert.___torch_mangle_1910.BertOutput = prim::GetAttr[name=\"output\"](%3655)\n",
      "  %5327 : __torch__.transformers.modeling_bert.___torch_mangle_1906.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3655)\n",
      "  %5328 : __torch__.transformers.modeling_bert.___torch_mangle_1904.BertAttention = prim::GetAttr[name=\"attention\"](%3655)\n",
      "  %5329 : __torch__.transformers.modeling_bert.___torch_mangle_1903.BertSelfOutput = prim::GetAttr[name=\"output\"](%5328)\n",
      "  %5330 : __torch__.transformers.modeling_bert.___torch_mangle_1899.BertSelfAttention = prim::GetAttr[name=\"self\"](%5328)\n",
      "  %5331 : __torch__.torch.nn.modules.dropout.___torch_mangle_1898.Dropout = prim::GetAttr[name=\"dropout\"](%5330)\n",
      "  %5332 : __torch__.torch.nn.modules.linear.___torch_mangle_1897.Linear = prim::GetAttr[name=\"value\"](%5330)\n",
      "  %5333 : __torch__.torch.nn.modules.linear.___torch_mangle_1896.Linear = prim::GetAttr[name=\"key\"](%5330)\n",
      "  %5334 : __torch__.torch.nn.modules.linear.___torch_mangle_1895.Linear = prim::GetAttr[name=\"query\"](%5330)\n",
      "  %5335 : Tensor = prim::GetAttr[name=\"bias\"](%5334)\n",
      "  %5336 : Tensor = prim::GetAttr[name=\"weight\"](%5334)\n",
      "  %5337 : Float(768, 768) = aten::t(%5336), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.61 : Float(1, 128, 768) = aten::matmul(%input.105, %5337), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.61 : Float(1, 128, 768) = aten::add_(%output.61, %5335, %5339), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5341 : Tensor = prim::GetAttr[name=\"bias\"](%5333)\n",
      "  %5342 : Tensor = prim::GetAttr[name=\"weight\"](%5333)\n",
      "  %5343 : Float(768, 768) = aten::t(%5342), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.62 : Float(1, 128, 768) = aten::matmul(%input.105, %5343), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5345 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.63 : Float(1, 128, 768) = aten::add_(%output.62, %5341, %5345), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5347 : Tensor = prim::GetAttr[name=\"bias\"](%5332)\n",
      "  %5348 : Tensor = prim::GetAttr[name=\"weight\"](%5332)\n",
      "  %5349 : Float(768, 768) = aten::t(%5348), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.63 : Float(1, 128, 768) = aten::matmul(%input.105, %5349), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5351 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.65 : Float(1, 128, 768) = aten::add_(%output.63, %5347, %5351), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5353 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5354 : int = aten::size(%x.61, %5353), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5355 : Long() = prim::NumToTensor(%5354), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5356 : int = aten::Int(%5355), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5357 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5358 : int = aten::size(%x.61, %5357), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5359 : Long() = prim::NumToTensor(%5358), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5360 : int = aten::Int(%5359), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5361 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5362 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5363 : int[] = prim::ListConstruct(%5356, %5360, %5361, %5362), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %x.62 : Float(1, 128, 12, 64) = aten::view(%x.61, %5363), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5365 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5366 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5367 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5368 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5369 : int[] = prim::ListConstruct(%5365, %5366, %5367, %5368), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %query_layer.11 : Float(1, 12, 128, 64) = aten::permute(%x.62, %5369), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5371 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5372 : int = aten::size(%x.63, %5371), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5373 : Long() = prim::NumToTensor(%5372), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5374 : int = aten::Int(%5373), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5375 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5376 : int = aten::size(%x.63, %5375), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5377 : Long() = prim::NumToTensor(%5376), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5378 : int = aten::Int(%5377), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5379 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5380 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5381 : int[] = prim::ListConstruct(%5374, %5378, %5379, %5380), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %x.64 : Float(1, 128, 12, 64) = aten::view(%x.63, %5381), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5383 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5384 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5386 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5387 : int[] = prim::ListConstruct(%5383, %5384, %5385, %5386), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %key_layer.11 : Float(1, 12, 128, 64) = aten::permute(%x.64, %5387), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5389 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5390 : int = aten::size(%x.65, %5389), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5391 : Long() = prim::NumToTensor(%5390), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5392 : int = aten::Int(%5391), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5393 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5394 : int = aten::size(%x.65, %5393), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5395 : Long() = prim::NumToTensor(%5394), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5396 : int = aten::Int(%5395), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5397 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5398 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5399 : int[] = prim::ListConstruct(%5392, %5396, %5397, %5398), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %x.66 : Float(1, 128, 12, 64) = aten::view(%x.65, %5399), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5401 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5402 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5403 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5404 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5405 : int[] = prim::ListConstruct(%5401, %5402, %5403, %5404), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %value_layer.11 : Float(1, 12, 128, 64) = aten::permute(%x.66, %5405), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5407 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5408 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5409 : Float(1, 12, 64, 128) = aten::transpose(%key_layer.11, %5407, %5408), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.21 : Float(1, 12, 128, 128) = aten::matmul(%query_layer.11, %5409), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5411 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores.22 : Float(1, 12, 128, 128) = aten::div(%attention_scores.21, %5411), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %5413 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.106 : Float(1, 12, 128, 128) = aten::add(%attention_scores.22, %attention_mask, %5413), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %5415 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %5416 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %input.107 : Float(1, 12, 128, 128) = aten::softmax(%input.106, %5415, %5416), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %5418 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5419 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs.11 : Float(1, 12, 128, 128) = aten::dropout(%input.107, %5418, %5419), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.21 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %5422 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5423 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5424 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5425 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5426 : int[] = prim::ListConstruct(%5422, %5423, %5424, %5425), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5427 : Float(1, 128, 12, 64) = aten::permute(%context_layer.21, %5426), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer.22 : Float(1, 128, 12, 64) = aten::contiguous(%5427, %5428), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5430 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5431 : int = aten::size(%context_layer.22, %5430), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5432 : Long() = prim::NumToTensor(%5431), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5433 : int = aten::Int(%5432), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5434 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5435 : int = aten::size(%context_layer.22, %5434), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5436 : Long() = prim::NumToTensor(%5435), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5437 : int = aten::Int(%5436), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %5438 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %5439 : int[] = prim::ListConstruct(%5433, %5437, %5438), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self\n",
      "  %input.108 : Float(1, 128, 768) = aten::view(%context_layer.22, %5439), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %5441 : __torch__.torch.nn.modules.normalization.___torch_mangle_1901.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%5329)\n",
      "  %5442 : __torch__.torch.nn.modules.dropout.___torch_mangle_1902.Dropout = prim::GetAttr[name=\"dropout\"](%5329)\n",
      "  %5443 : __torch__.torch.nn.modules.linear.___torch_mangle_1900.Linear = prim::GetAttr[name=\"dense\"](%5329)\n",
      "  %5444 : Tensor = prim::GetAttr[name=\"bias\"](%5443)\n",
      "  %5445 : Tensor = prim::GetAttr[name=\"weight\"](%5443)\n",
      "  %5446 : Float(768, 768) = aten::t(%5445), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.64 : Float(1, 128, 768) = aten::matmul(%input.108, %5446), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5448 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.109 : Float(1, 128, 768) = aten::add_(%output.64, %5444, %5448), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5450 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5451 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.21 : Float(1, 128, 768) = aten::dropout(%input.109, %5450, %5451), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5453 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.110 : Float(1, 128, 768) = aten::add(%hidden_states.21, %input.105, %5453), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %5455 : Tensor = prim::GetAttr[name=\"bias\"](%5441)\n",
      "  %5456 : Tensor = prim::GetAttr[name=\"weight\"](%5441)\n",
      "  %5457 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5458 : int[] = prim::ListConstruct(%5457), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm\n",
      "  %5459 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5460 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor.11 : Float(1, 128, 768) = aten::layer_norm(%input.110, %5458, %5456, %5455, %5459, %5460), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5462 : __torch__.torch.nn.modules.linear.___torch_mangle_1905.Linear = prim::GetAttr[name=\"dense\"](%5327)\n",
      "  %5463 : Tensor = prim::GetAttr[name=\"bias\"](%5462)\n",
      "  %5464 : Tensor = prim::GetAttr[name=\"weight\"](%5462)\n",
      "  %5465 : Float(768, 3072) = aten::t(%5464), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.65 : Float(1, 128, 3072) = aten::matmul(%input_tensor.11, %5465), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5467 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.111 : Float(1, 128, 3072) = aten::add_(%output.65, %5463, %5467), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.112 : Float(1, 128, 3072) = aten::gelu(%input.111), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %5470 : __torch__.torch.nn.modules.normalization.___torch_mangle_1908.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%5326)\n",
      "  %5471 : __torch__.torch.nn.modules.dropout.___torch_mangle_1909.Dropout = prim::GetAttr[name=\"dropout\"](%5326)\n",
      "  %5472 : __torch__.torch.nn.modules.linear.___torch_mangle_1907.Linear = prim::GetAttr[name=\"dense\"](%5326)\n",
      "  %5473 : Tensor = prim::GetAttr[name=\"bias\"](%5472)\n",
      "  %5474 : Tensor = prim::GetAttr[name=\"weight\"](%5472)\n",
      "  %5475 : Float(3072, 768) = aten::t(%5474), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.66 : Float(1, 128, 768) = aten::matmul(%input.112, %5475), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5477 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.113 : Float(1, 128, 768) = aten::add_(%output.66, %5473, %5477), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5479 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5480 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.22 : Float(1, 128, 768) = aten::dropout(%input.113, %5479, %5480), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5482 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.114 : Float(1, 128, 768) = aten::add(%hidden_states.22, %input_tensor.11, %5482), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %5484 : Tensor = prim::GetAttr[name=\"bias\"](%5470)\n",
      "  %5485 : Tensor = prim::GetAttr[name=\"weight\"](%5470)\n",
      "  %5486 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5487 : int[] = prim::ListConstruct(%5486), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm\n",
      "  %5488 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5489 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input.115 : Float(1, 128, 768) = aten::layer_norm(%input.114, %5487, %5485, %5484, %5488, %5489), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5491 : __torch__.transformers.modeling_bert.___torch_mangle_1927.BertOutput = prim::GetAttr[name=\"output\"](%3653)\n",
      "  %5492 : __torch__.transformers.modeling_bert.___torch_mangle_1923.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%3653)\n",
      "  %5493 : __torch__.transformers.modeling_bert.___torch_mangle_1921.BertAttention = prim::GetAttr[name=\"attention\"](%3653)\n",
      "  %5494 : __torch__.transformers.modeling_bert.___torch_mangle_1920.BertSelfOutput = prim::GetAttr[name=\"output\"](%5493)\n",
      "  %5495 : __torch__.transformers.modeling_bert.___torch_mangle_1916.BertSelfAttention = prim::GetAttr[name=\"self\"](%5493)\n",
      "  %5496 : __torch__.torch.nn.modules.dropout.___torch_mangle_1915.Dropout = prim::GetAttr[name=\"dropout\"](%5495)\n",
      "  %5497 : __torch__.torch.nn.modules.linear.___torch_mangle_1914.Linear = prim::GetAttr[name=\"value\"](%5495)\n",
      "  %5498 : __torch__.torch.nn.modules.linear.___torch_mangle_1913.Linear = prim::GetAttr[name=\"key\"](%5495)\n",
      "  %5499 : __torch__.torch.nn.modules.linear.___torch_mangle_1912.Linear = prim::GetAttr[name=\"query\"](%5495)\n",
      "  %5500 : Tensor = prim::GetAttr[name=\"bias\"](%5499)\n",
      "  %5501 : Tensor = prim::GetAttr[name=\"weight\"](%5499)\n",
      "  %5502 : Float(768, 768) = aten::t(%5501), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.67 : Float(1, 128, 768) = aten::matmul(%input.115, %5502), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5504 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.67 : Float(1, 128, 768) = aten::add_(%output.67, %5500, %5504), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5506 : Tensor = prim::GetAttr[name=\"bias\"](%5498)\n",
      "  %5507 : Tensor = prim::GetAttr[name=\"weight\"](%5498)\n",
      "  %5508 : Float(768, 768) = aten::t(%5507), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.68 : Float(1, 128, 768) = aten::matmul(%input.115, %5508), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5510 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.69 : Float(1, 128, 768) = aten::add_(%output.68, %5506, %5510), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5512 : Tensor = prim::GetAttr[name=\"bias\"](%5497)\n",
      "  %5513 : Tensor = prim::GetAttr[name=\"weight\"](%5497)\n",
      "  %5514 : Float(768, 768) = aten::t(%5513), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.69 : Float(1, 128, 768) = aten::matmul(%input.115, %5514), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5516 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %x.71 : Float(1, 128, 768) = aten::add_(%output.69, %5512, %5516), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5518 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5519 : int = aten::size(%x.67, %5518), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5520 : Long() = prim::NumToTensor(%5519), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5521 : int = aten::Int(%5520), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5522 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5523 : int = aten::size(%x.67, %5522), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5524 : Long() = prim::NumToTensor(%5523), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5525 : int = aten::Int(%5524), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5526 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5527 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5528 : int[] = prim::ListConstruct(%5521, %5525, %5526, %5527), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %x.68 : Float(1, 128, 12, 64) = aten::view(%x.67, %5528), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5530 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5531 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5532 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5533 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5534 : int[] = prim::ListConstruct(%5530, %5531, %5532, %5533), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %query_layer : Float(1, 12, 128, 64) = aten::permute(%x.68, %5534), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5536 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5537 : int = aten::size(%x.69, %5536), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5538 : Long() = prim::NumToTensor(%5537), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5539 : int = aten::Int(%5538), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5541 : int = aten::size(%x.69, %5540), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5542 : Long() = prim::NumToTensor(%5541), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5543 : int = aten::Int(%5542), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5544 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5545 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5546 : int[] = prim::ListConstruct(%5539, %5543, %5544, %5545), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %x.70 : Float(1, 128, 12, 64) = aten::view(%x.69, %5546), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5548 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5549 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5551 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5552 : int[] = prim::ListConstruct(%5548, %5549, %5550, %5551), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %key_layer : Float(1, 12, 128, 64) = aten::permute(%x.70, %5552), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5554 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5555 : int = aten::size(%x.71, %5554), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5556 : Long() = prim::NumToTensor(%5555), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5557 : int = aten::Int(%5556), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5558 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5559 : int = aten::size(%x.71, %5558), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:237:0\n",
      "  %5560 : Long() = prim::NumToTensor(%5559), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5561 : int = aten::Int(%5560), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5562 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5563 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5564 : int[] = prim::ListConstruct(%5557, %5561, %5562, %5563), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %x : Float(1, 128, 12, 64) = aten::view(%x.71, %5564), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:238:0\n",
      "  %5566 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5567 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5568 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5569 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5570 : int[] = prim::ListConstruct(%5566, %5567, %5568, %5569), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %value_layer : Float(1, 12, 128, 64) = aten::permute(%x, %5570), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:239:0\n",
      "  %5572 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5573 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5574 : Float(1, 12, 64, 128) = aten::transpose(%key_layer, %5572, %5573), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %attention_scores.23 : Float(1, 12, 128, 128) = aten::matmul(%query_layer, %5574), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:268:0\n",
      "  %5576 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %attention_scores : Float(1, 12, 128, 128) = aten::div(%attention_scores.23, %5576), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:269:0\n",
      "  %5578 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %input.116 : Float(1, 12, 128, 128) = aten::add(%attention_scores, %attention_mask, %5578), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:272:0\n",
      "  %5580 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %5581 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %input.117 : Float(1, 12, 128, 128) = aten::softmax(%input.116, %5580, %5581), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1442:0\n",
      "  %5583 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5584 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %attention_probs : Float(1, 12, 128, 128) = aten::dropout(%input.117, %5583, %5584), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %context_layer.23 : Float(1, 12, 128, 64) = aten::matmul(%attention_probs, %value_layer), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:285:0\n",
      "  %5587 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5588 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5589 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5590 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5591 : int[] = prim::ListConstruct(%5587, %5588, %5589, %5590), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5592 : Float(1, 128, 12, 64) = aten::permute(%context_layer.23, %5591), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5593 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %context_layer : Float(1, 128, 12, 64) = aten::contiguous(%5592, %5593), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:287:0\n",
      "  %5595 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5596 : int = aten::size(%context_layer, %5595), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5597 : Long() = prim::NumToTensor(%5596), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5598 : int = aten::Int(%5597), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5599 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5600 : int = aten::size(%context_layer, %5599), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:288:0\n",
      "  %5601 : Long() = prim::NumToTensor(%5600), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5602 : int = aten::Int(%5601), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %5603 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %5604 : int[] = prim::ListConstruct(%5598, %5602, %5603), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self\n",
      "  %input.118 : Float(1, 128, 768) = aten::view(%context_layer, %5604), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:289:0\n",
      "  %5606 : __torch__.torch.nn.modules.normalization.___torch_mangle_1918.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%5494)\n",
      "  %5607 : __torch__.torch.nn.modules.dropout.___torch_mangle_1919.Dropout = prim::GetAttr[name=\"dropout\"](%5494)\n",
      "  %5608 : __torch__.torch.nn.modules.linear.___torch_mangle_1917.Linear = prim::GetAttr[name=\"dense\"](%5494)\n",
      "  %5609 : Tensor = prim::GetAttr[name=\"bias\"](%5608)\n",
      "  %5610 : Tensor = prim::GetAttr[name=\"weight\"](%5608)\n",
      "  %5611 : Float(768, 768) = aten::t(%5610), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.70 : Float(1, 128, 768) = aten::matmul(%input.118, %5611), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5613 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.119 : Float(1, 128, 768) = aten::add_(%output.70, %5609, %5613), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5615 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5616 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.23 : Float(1, 128, 768) = aten::dropout(%input.119, %5615, %5616), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5618 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %input.120 : Float(1, 128, 768) = aten::add(%hidden_states.23, %input.115, %5618), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:305:0\n",
      "  %5620 : Tensor = prim::GetAttr[name=\"bias\"](%5606)\n",
      "  %5621 : Tensor = prim::GetAttr[name=\"weight\"](%5606)\n",
      "  %5622 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5623 : int[] = prim::ListConstruct(%5622), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm\n",
      "  %5624 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5625 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %input_tensor : Float(1, 128, 768) = aten::layer_norm(%input.120, %5623, %5621, %5620, %5624, %5625), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5627 : __torch__.torch.nn.modules.linear.___torch_mangle_1922.Linear = prim::GetAttr[name=\"dense\"](%5492)\n",
      "  %5628 : Tensor = prim::GetAttr[name=\"bias\"](%5627)\n",
      "  %5629 : Tensor = prim::GetAttr[name=\"weight\"](%5627)\n",
      "  %5630 : Float(768, 3072) = aten::t(%5629), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output.71 : Float(1, 128, 3072) = aten::matmul(%input_tensor, %5630), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5632 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.121 : Float(1, 128, 3072) = aten::add_(%output.71, %5628, %5632), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.122 : Float(1, 128, 3072) = aten::gelu(%input.121), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1313:0\n",
      "  %5635 : __torch__.torch.nn.modules.normalization.___torch_mangle_1925.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%5491)\n",
      "  %5636 : __torch__.torch.nn.modules.dropout.___torch_mangle_1926.Dropout = prim::GetAttr[name=\"dropout\"](%5491)\n",
      "  %5637 : __torch__.torch.nn.modules.linear.___torch_mangle_1924.Linear = prim::GetAttr[name=\"dense\"](%5491)\n",
      "  %5638 : Tensor = prim::GetAttr[name=\"bias\"](%5637)\n",
      "  %5639 : Tensor = prim::GetAttr[name=\"weight\"](%5637)\n",
      "  %5640 : Float(3072, 768) = aten::t(%5639), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %output : Float(1, 128, 768) = aten::matmul(%input.122, %5640), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1612:0\n",
      "  %5642 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %input.123 : Float(1, 128, 768) = aten::add_(%output, %5638, %5642), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1614:0\n",
      "  %5644 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5645 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %hidden_states.24 : Float(1, 128, 768) = aten::dropout(%input.123, %5644, %5645), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dropout # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:936:0\n",
      "  %5647 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %input.124 : Float(1, 128, 768) = aten::add(%hidden_states.24, %input_tensor, %5647), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:381:0\n",
      "  %5649 : Tensor = prim::GetAttr[name=\"bias\"](%5635)\n",
      "  %5650 : Tensor = prim::GetAttr[name=\"weight\"](%5635)\n",
      "  %5651 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5652 : int[] = prim::ListConstruct(%5651), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm\n",
      "  %5653 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5654 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %hidden_states : Float(1, 128, 768) = aten::layer_norm(%input.124, %5652, %5650, %5649, %5653, %5654), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1956:0\n",
      "  %5656 : __torch__.torch.nn.modules.activation.___torch_mangle_1932.Tanh = prim::GetAttr[name=\"activation\"](%3386)\n",
      "  %5657 : __torch__.torch.nn.modules.linear.___torch_mangle_1931.Linear = prim::GetAttr[name=\"dense\"](%3386)\n",
      "  %5658 : int = prim::Constant[value=0](), scope: __module.pooler # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:515:0\n",
      "  %5659 : int = prim::Constant[value=0](), scope: __module.pooler # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:515:0\n",
      "  %5660 : int = prim::Constant[value=9223372036854775807](), scope: __module.pooler # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:515:0\n",
      "  %5661 : int = prim::Constant[value=1](), scope: __module.pooler # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:515:0\n",
      "  %5662 : Float(1, 128, 768) = aten::slice(%hidden_states, %5658, %5659, %5660, %5661), scope: __module.pooler # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:515:0\n",
      "  %5663 : int = prim::Constant[value=1](), scope: __module.pooler # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:515:0\n",
      "  %5664 : int = prim::Constant[value=0](), scope: __module.pooler # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:515:0\n",
      "  %input.125 : Float(1, 768) = aten::select(%5662, %5663, %5664), scope: __module.pooler # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py:515:0\n",
      "  %5666 : Tensor = prim::GetAttr[name=\"bias\"](%5657)\n",
      "  %5667 : Tensor = prim::GetAttr[name=\"weight\"](%5657)\n",
      "  %5668 : Float(768, 768) = aten::t(%5667), scope: __module.pooler/__module.pooler.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1610:0\n",
      "  %5669 : int = prim::Constant[value=1](), scope: __module.pooler/__module.pooler.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1610:0\n",
      "  %5670 : int = prim::Constant[value=1](), scope: __module.pooler/__module.pooler.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1610:0\n",
      "  %input : Float(1, 768) = aten::addmm(%5666, %input.125, %5668, %5669, %5670), scope: __module.pooler/__module.pooler.dense # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1610:0\n",
      "  %5672 : Float(1, 768) = aten::tanh(%input), scope: __module.pooler/__module.pooler.activation # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/activation.py:318:0\n",
      "  %2746 : (Float(1, 128, 768), Float(1, 768)) = prim::TupleConstruct(%hidden_states, %5672)\n",
      "  return (%2746)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuron\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "sentence1=\"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"\n",
    "sentence2=\"The greatest glory in living lies not in never falling, but in rising every time we fall.\"\n",
    "sentence3=\"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success. The greatest glory in living lies not in never falling, but in rising every time we fall. If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"\n",
    "\n",
    "sentence1=\"The animal didn't cross the street because it was too tired.\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenize_sentence = tokenizer.tokenize(sentence1)\n",
    "\n",
    "encoded_sentence = tokenizer.encode_plus(sentence1, sentence3, max_length=128, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "example_inputs = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids']\n",
    "\n",
    "\n",
    "traced = torch.jit.trace( model, example_inputs )\n",
    "torch._C._jit_pass_inline(traced.graph)\n",
    "print( \"=== Pre compile graph\")\n",
    "print( traced.graph )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pre compile graph\n",
      "graph(%self : __torch__.torch_neuron.convert.AwsNeuronGraphModule,\n",
      "      %tensor.1 : Tensor,\n",
      "      %tensor0.1 : Tensor,\n",
      "      %argument_3.1 : Tensor):\n",
      "  %129 : Function = prim::Constant[name=\"neuron_function\"]()\n",
      "  %117 : Tensor = prim::Constant[value=<Tensor>]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %111 : Tensor = prim::Constant[value=<Tensor>]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %106 : Tensor = prim::Constant[value=<Tensor>]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %89 : Tensor = prim::Constant[value=<Tensor>]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:25:0\n",
      "  %61 : Tensor = prim::Constant[value={-10000}]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %56 : None = prim::Constant() # :0:0\n",
      "  %54 : bool = prim::Constant[value=0]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:63:0\n",
      "  %5 : int = prim::Constant[value=0]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %6 : int = prim::Constant[value=9223372036854775807]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %7 : int = prim::Constant[value=1]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %17 : int = prim::Constant[value=2]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %26 : int = prim::Constant[value=3]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %53 : int = prim::Constant[value=6]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:63:0\n",
      "  %59 : float = prim::Constant[value=1.]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %67 : int = prim::Constant[value=4]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/prim.py:41:0\n",
      "  %113 : int = prim::Constant[value=-1]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %8 : Tensor = aten::slice(%tensor0.1, %5, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %tensor1.1 : Tensor = aten::slice(%8, %7, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %12 : Tensor = aten::slice(%tensor1.1, %5, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %14 : Tensor = aten::slice(%12, %7, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %16 : Tensor = aten::unsqueeze(%14, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %tensor2.1 : Tensor = aten::unsqueeze(%16, %17) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %20 : Tensor = aten::slice(%tensor2.1, %5, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %22 : Tensor = aten::slice(%20, %7, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %24 : Tensor = aten::slice(%22, %17, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %tensor3.1 : Tensor = aten::slice(%24, %26, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %29 : Tensor = aten::slice(%tensor3.1, %5, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %31 : Tensor = aten::slice(%29, %7, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %33 : Tensor = aten::slice(%31, %17, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %tensor4.1 : Tensor = aten::slice(%33, %26, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %37 : Tensor = aten::slice(%tensor4.1, %5, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %39 : Tensor = aten::slice(%37, %7, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %41 : Tensor = aten::slice(%39, %17, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %tensor5.1 : Tensor = aten::slice(%41, %26, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %45 : Tensor = aten::slice(%tensor5.1, %5, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %47 : Tensor = aten::slice(%45, %7, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %49 : Tensor = aten::slice(%47, %17, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %tensor6.1 : Tensor = aten::slice(%49, %26, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %57 : Tensor = aten::to(%tensor6.1, %53, %54, %54, %56) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:63:0\n",
      "  %60 : Tensor = aten::rsub(%57, %59, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %62 : Tensor = aten::mul(%60, %61) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %64 : int = aten::size(%tensor.1, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:49:0\n",
      "  %num.1 : Tensor = prim::NumToTensor(%64) # :0:0\n",
      "  %tensor7.1 : Tensor = aten::to(%num.1, %67, %54, %54, %56) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/prim.py:41:0\n",
      "  %tensor8.1 : Tensor = aten::to(%tensor7.1, %67, %54, %54, %56) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:63:0\n",
      "  %tensor9.1 : Tensor = aten::to(%tensor8.1, %26, %54, %54, %56) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:63:0\n",
      "  %end.1 : Tensor = aten::to(%tensor9.1, %26, %54, %54, %56) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:31:0\n",
      "  %88 : int = aten::Int(%end.1)\n",
      "  %90 : Tensor = aten::slice(%89, %5, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %tensor10.1 : Tensor = aten::slice(%90, %7, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %94 : Tensor = aten::slice(%tensor10.1, %5, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %tensor11.1 : Tensor = aten::slice(%94, %7, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %98 : Tensor = aten::slice(%tensor11.1, %5, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %tensor12.1 : Tensor = aten::slice(%98, %7, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %102 : Tensor = aten::slice(%tensor12.1, %5, %5, %6, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %105 : Tensor = aten::slice(%102, %7, %5, %88, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/native_ops/aten.py:27:0\n",
      "  %110 : Tensor = aten::embedding(%106, %tensor.1, %5, %54, %54) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %116 : Tensor = aten::embedding(%111, %105, %113, %54, %54) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %121 : Tensor = aten::embedding(%117, %argument_3.1, %113, %54, %54) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %124 : Tensor = aten::add(%110, %116, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %126 : Tensor = aten::add(%124, %121, %7) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/resolve_function.py:52:0\n",
      "  %145 : Tensor = prim::Constant[value=<Tensor>]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/decorators.py:252:0\n",
      "  %146 : Tensor = prim::Constant[value=<Tensor>]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/decorators.py:252:0\n",
      "  %147 : Tensor = prim::Constant[value=<Tensor>]() # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/decorators.py:252:0\n",
      "  %148 : Tensor[] = prim::ListConstruct(%126, %62)\n",
      "  %149 : Tensor, %150 : Tensor = neuron::forward_2(%148, %147, %146, %145) # /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch_neuron/decorators.py:252:0\n",
      "  %151 : (Tensor, Tensor) = prim::TupleConstruct(%149, %150)\n",
      "  %131 : Tensor, %132 : Tensor = prim::TupleUnpack(%151)\n",
      "  %135 : (Tensor, Tensor) = prim::TupleConstruct(%131, %132)\n",
      "  return (%135)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_neuron = torch.jit.load(\"neuron_compiled_bert_model.pt\")\n",
    "torch._C._jit_pass_inline(traced.graph)\n",
    "traced = torch.jit.trace( model_neuron, example_inputs )\n",
    "print( \"=== Post compile graph\")\n",
    "print( traced.graph )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'animal',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'cross',\n",
       " 'the',\n",
       " 'street',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'too',\n",
       " 'tired',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1996, 4111, 2134, 1005, 1056, 2892, 1996, 2395, 2138, 2009, 2001,\n",
       "         2205, 5458, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentence.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "out = model(encoded_sentence,return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5109,  0.4340,  0.3485,  ..., -0.4597,  0.4636, -0.7299],\n",
       "         [-0.1760, -0.1786, -0.9261,  ...,  0.2803,  1.2572, -0.3424],\n",
       "         [ 0.0323, -0.0284, -0.1806,  ..., -0.4487,  0.1431,  0.0424],\n",
       "         ...,\n",
       "         [ 0.3229, -0.2139,  0.8898,  ..., -0.8492, -0.0889, -1.1546],\n",
       "         [ 0.2862, -0.0026,  0.8554,  ..., -0.7923, -0.2182, -1.1506],\n",
       "         [ 0.2518,  0.1158,  0.7950,  ..., -0.7763, -0.2980, -1.3862]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
